package handlers

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"log"
	"net/http"
	"strings"
	"time"

	"github.com/JillVernus/cc-bridge/internal/apikey"
	"github.com/JillVernus/cc-bridge/internal/config"
	"github.com/JillVernus/cc-bridge/internal/httpclient"
	"github.com/JillVernus/cc-bridge/internal/middleware"
	"github.com/JillVernus/cc-bridge/internal/pricing"
	"github.com/JillVernus/cc-bridge/internal/providers"
	"github.com/JillVernus/cc-bridge/internal/quota"
	"github.com/JillVernus/cc-bridge/internal/requestlog"
	"github.com/JillVernus/cc-bridge/internal/scheduler"
	"github.com/JillVernus/cc-bridge/internal/types"
	"github.com/JillVernus/cc-bridge/internal/utils"
	"github.com/gin-gonic/gin"
)

// ProxyHandler ä»£ç†å¤„ç†å™¨
// æ”¯æŒå¤šæ¸ é“è°ƒåº¦ï¼šå½“é…ç½®å¤šä¸ªæ¸ é“æ—¶è‡ªåŠ¨å¯ç”¨
func ProxyHandler(envCfg *config.EnvConfig, cfgManager *config.ConfigManager, channelScheduler *scheduler.ChannelScheduler, reqLogManager *requestlog.Manager) gin.HandlerFunc {
	return ProxyHandlerWithAPIKey(envCfg, cfgManager, channelScheduler, reqLogManager, nil, nil, nil)
}

// ProxyHandlerWithAPIKey ä»£ç†å¤„ç†å™¨ï¼ˆæ”¯æŒ API Key éªŒè¯ï¼‰
func ProxyHandlerWithAPIKey(envCfg *config.EnvConfig, cfgManager *config.ConfigManager, channelScheduler *scheduler.ChannelScheduler, reqLogManager *requestlog.Manager, apiKeyManager *apikey.Manager, usageManager *quota.UsageManager, failoverTracker *config.FailoverTracker) gin.HandlerFunc {
	return gin.HandlerFunc(func(c *gin.Context) {
		// å…ˆè¿›è¡Œè®¤è¯ï¼ˆå¦‚æœä¸Šæ¸¸ä¸­é—´ä»¶å°šæœªå®Œæˆè®¤è¯ï¼‰
		if _, exists := c.Get(middleware.ContextKeyAPIKeyID); !exists {
			middleware.ProxyAuthMiddlewareWithAPIKey(envCfg, apiKeyManager)(c)
			if c.IsAborted() {
				return
			}
		}

		// Check endpoint permission
		if vk, exists := c.Get(middleware.ContextKeyValidatedKey); exists {
			if validatedKey, ok := vk.(*apikey.ValidatedKey); ok && validatedKey != nil {
				if !validatedKey.CheckEndpointPermission("messages") {
					c.JSON(403, gin.H{
						"error": "Endpoint /v1/messages not allowed for this API key",
						"code":  "ENDPOINT_NOT_ALLOWED",
					})
					return
				}
			}
		}

		startTime := time.Now()

		// è¯»å–åŸå§‹è¯·æ±‚ä½“
		maxBodyMB := envCfg.MaxRequestBodyMB
		if maxBodyMB <= 0 {
			maxBodyMB = 20
		}
		c.Request.Body = http.MaxBytesReader(c.Writer, c.Request.Body, int64(maxBodyMB)*1024*1024)

		bodyBytes, err := io.ReadAll(c.Request.Body)
		if err != nil {
			var maxBytesErr *http.MaxBytesError
			if errors.As(err, &maxBytesErr) {
				c.JSON(http.StatusRequestEntityTooLarge, gin.H{
					"error":   "Request body too large",
					"message": fmt.Sprintf("Maximum allowed request size is %d MB", maxBodyMB),
				})
				return
			}
			c.JSON(400, gin.H{"error": "Failed to read request body"})
			return
		}
		// æ¢å¤è¯·æ±‚ä½“ä¾›åç»­ä½¿ç”¨
		c.Request.Body = io.NopCloser(bytes.NewReader(bodyBytes))

		// Store request data for debug logging
		StoreDebugRequestData(c, bodyBytes)

		// claudeReq å˜é‡ç”¨äºåˆ¤æ–­æ˜¯å¦æµå¼è¯·æ±‚å’Œæå– user_id
		var claudeReq types.ClaudeRequest
		if len(bodyBytes) > 0 {
			_ = json.Unmarshal(bodyBytes, &claudeReq)
		}

		// Check model permission
		if vk, exists := c.Get(middleware.ContextKeyValidatedKey); exists {
			if validatedKey, ok := vk.(*apikey.ValidatedKey); ok && validatedKey != nil {
				if !validatedKey.CheckModelPermission(claudeReq.Model) {
					c.JSON(403, gin.H{
						"error": fmt.Sprintf("Model %s not allowed for this API key", claudeReq.Model),
						"code":  "MODEL_NOT_ALLOWED",
					})
					return
				}
			}
		}

		// æå– user_id ç”¨äº Trace äº²å’Œæ€§
		compoundUserID := extractUserID(bodyBytes)
		userID, sessionID := parseClaudeCodeUserID(compoundUserID)

		// æå– API Key ID ç”¨äºè¯·æ±‚æ—¥å¿— (nil è¡¨ç¤ºæœªè®¾ç½®)
		var apiKeyID *int64
		if id, exists := c.Get(middleware.ContextKeyAPIKeyID); exists {
			if idVal, ok := id.(int64); ok {
				apiKeyID = &idVal
			}
		}

		// åˆ›å»º pending è¯·æ±‚æ—¥å¿—è®°å½•
		var requestLogID string
		if reqLogManager != nil {
			pendingLog := &requestlog.RequestLog{
				Status:      requestlog.StatusPending,
				InitialTime: startTime,
				Model:       claudeReq.Model,
				Stream:      claudeReq.Stream,
				Endpoint:    "/v1/messages",
				ClientID:    userID,
				SessionID:   sessionID,
				APIKeyID:    apiKeyID,
			}
			if err := reqLogManager.Add(pendingLog); err != nil {
				log.Printf("âš ï¸ åˆ›å»º pending è¯·æ±‚æ—¥å¿—å¤±è´¥: %v", err)
			} else {
				requestLogID = pendingLog.ID
			}
		}

		// æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ¸ é“æ¨¡å¼
		isMultiChannel := channelScheduler.IsMultiChannelMode(false)

		// Get allowed channels from API key permissions
		var allowedChannels []int
		if vk, exists := c.Get(middleware.ContextKeyValidatedKey); exists {
			if validatedKey, ok := vk.(*apikey.ValidatedKey); ok && validatedKey != nil {
				allowedChannels = validatedKey.GetAllowedChannels(false) // false = Messages API
			}
		}

		if isMultiChannel {
			// å¤šæ¸ é“æ¨¡å¼ï¼šä½¿ç”¨è°ƒåº¦å™¨
			handleMultiChannelProxy(c, envCfg, cfgManager, channelScheduler, bodyBytes, claudeReq, userID, sessionID, apiKeyID, startTime, reqLogManager, requestLogID, usageManager, allowedChannels, failoverTracker)
		} else {
			// å•æ¸ é“æ¨¡å¼ï¼šä½¿ç”¨ç°æœ‰é€»è¾‘
			handleSingleChannelProxy(c, envCfg, cfgManager, bodyBytes, claudeReq, startTime, reqLogManager, requestLogID, usageManager, allowedChannels, failoverTracker, userID, sessionID, apiKeyID)
		}
	})
}

// extractUserID ä»è¯·æ±‚ä½“ä¸­æå– user_id
func extractUserID(bodyBytes []byte) string {
	var req struct {
		Metadata struct {
			UserID string `json:"user_id"`
		} `json:"metadata"`
	}
	if err := json.Unmarshal(bodyBytes, &req); err == nil {
		return req.Metadata.UserID
	}
	return ""
}

// parseClaudeCodeUserID è§£æ Claude Code çš„å¤åˆ user_id æ ¼å¼
// æ ¼å¼: user_<hash>_account__session_<session_uuid>
// è¿”å›: (userID, sessionID)
func parseClaudeCodeUserID(compoundUserID string) (userID string, sessionID string) {
	compoundUserID = strings.TrimSpace(compoundUserID)
	if compoundUserID == "" {
		return "", ""
	}

	// æŸ¥æ‰¾åˆ†éš”ç¬¦ "_account__session_"
	const delimiter = "_account__session_"
	idx := strings.Index(compoundUserID, delimiter)
	if idx == -1 {
		// æ²¡æœ‰æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œæ•´ä¸ªå­—ç¬¦ä¸²ä½œä¸º userID
		return compoundUserID, ""
	}

	userID = strings.TrimSpace(compoundUserID[:idx])
	sessionID = strings.TrimSpace(compoundUserID[idx+len(delimiter):])
	return userID, sessionID
}

// handleMultiChannelProxy handles multi-channel proxy requests with failover support.
// When a channel fails and there are more channels to try, it logs the failed attempt
// with StatusFailover and creates a new pending log for the next attempt.
func handleMultiChannelProxy(
	c *gin.Context,
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	channelScheduler *scheduler.ChannelScheduler,
	bodyBytes []byte,
	claudeReq types.ClaudeRequest,
	clientID string,
	sessionID string,
	apiKeyID *int64,
	startTime time.Time,
	reqLogManager *requestlog.Manager,
	requestLogID string,
	usageManager *quota.UsageManager,
	allowedChannels []int,
	failoverTracker *config.FailoverTracker,
) {
	failedChannels := make(map[int]bool)
	var lastError error
	var lastFailoverError *struct {
		Status       int
		Body         []byte
		FailoverInfo string
	}
	var lastFailedUpstream *config.UpstreamConfig

	// Get active channel count as max retry attempts
	maxChannelAttempts := channelScheduler.GetActiveChannelCount(false)

	for channelAttempt := 0; channelAttempt < maxChannelAttempts; channelAttempt++ {
		// Select channel using scheduler
		selection, err := channelScheduler.SelectChannel(c.Request.Context(), clientID, failedChannels, false, allowedChannels)
		if err != nil {
			lastError = err
			break
		}

		upstream := selection.Upstream
		channelIndex := selection.ChannelIndex

		if envCfg.ShouldLog("info") {
			log.Printf("ğŸ¯ [Multi-Channel] Selected channel: [%d] %s (reason: %s, attempt %d/%d)",
				channelIndex, upstream.Name, selection.Reason, channelAttempt+1, maxChannelAttempts)
		}

		// Try all keys for this channel
		success, failoverErr, updatedLogID := tryChannelWithAllKeys(c, envCfg, cfgManager, upstream, bodyBytes, claudeReq, startTime, reqLogManager, requestLogID, usageManager, failoverTracker, clientID, sessionID, apiKeyID)
		requestLogID = updatedLogID // Update requestLogID in case it was changed during retry_wait

		if success {
			// Success: record and update trace affinity
			channelScheduler.RecordSuccess(channelIndex, false)
			channelScheduler.SetTraceAffinity(clientID, channelIndex)
			return
		}

		// Channel failed: record failure metrics
		channelScheduler.RecordFailure(channelIndex, false)
		failedChannels[channelIndex] = true

		// Check if there are more channels to try
		hasMoreChannels := channelAttempt < maxChannelAttempts-1 && len(failedChannels) < maxChannelAttempts

		if hasMoreChannels {
			// Failover case: log this failed attempt and create new pending log for next attempt
			if reqLogManager != nil && requestLogID != "" {
				completeTime := time.Now()
				httpStatus := 0
				upstreamErr := ""
				failoverInfo := ""
				if failoverErr != nil {
					httpStatus = failoverErr.Status
					upstreamErr = string(failoverErr.Body)
					failoverInfo = failoverErr.FailoverInfo
				}

				// Update current log as failover (keeping original error info)
				// Build error message with HTTP status for better visibility
				errorMsg := fmt.Sprintf("failover to next channel (%d/%d)", channelAttempt+1, maxChannelAttempts)
				if httpStatus > 0 {
					errorMsg = fmt.Sprintf("%d: %s", httpStatus, errorMsg)
				}

				failoverRecord := &requestlog.RequestLog{
					Status:        requestlog.StatusFailover,
					CompleteTime:  completeTime,
					DurationMs:    completeTime.Sub(startTime).Milliseconds(),
					Type:          upstream.ServiceType,
					ProviderName:  upstream.Name,
					Model:         claudeReq.Model,
					ChannelID:     channelIndex,
					ChannelName:   upstream.Name,
					HTTPStatus:    httpStatus,
					Error:         errorMsg,
					UpstreamError: upstreamErr,
					FailoverInfo:  failoverInfo,
				}
				if err := reqLogManager.Update(requestLogID, failoverRecord); err != nil {
					log.Printf("âš ï¸ Failed to update failover log: %v", err)
				}

				// Create new pending log for next channel attempt
				newPendingLog := &requestlog.RequestLog{
					Status:      requestlog.StatusPending,
					InitialTime: time.Now(),
					Model:       claudeReq.Model,
					Stream:      claudeReq.Stream,
					Endpoint:    "/v1/messages",
					ClientID:    clientID,
					SessionID:   sessionID,
					APIKeyID:    apiKeyID,
				}
				if err := reqLogManager.Add(newPendingLog); err != nil {
					log.Printf("âš ï¸ Failed to create failover pending log: %v", err)
				} else {
					requestLogID = newPendingLog.ID
					startTime = newPendingLog.InitialTime
				}
			}

			log.Printf("âš ï¸ [Multi-Channel] Channel [%d] %s all keys failed, trying next channel", channelIndex, upstream.Name)
		}

		if failoverErr != nil {
			lastFailoverError = failoverErr
			lastError = fmt.Errorf("channel [%d] %s failed", channelIndex, upstream.Name)
			lastFailedUpstream = upstream
		}
	}

	// All channels failed
	log.Printf("ğŸ’¥ [Multi-Channel] All channels failed")

	// Update request log with final error status (this is the last attempt, no more failovers)
	if reqLogManager != nil && requestLogID != "" {
		httpStatus := 503
		errMsg := "all channels unavailable"
		upstreamErr := ""
		failoverInfo := ""
		if lastFailoverError != nil && lastFailoverError.Status != 0 {
			httpStatus = lastFailoverError.Status
			upstreamErr = string(lastFailoverError.Body)
			failoverInfo = lastFailoverError.FailoverInfo
		}
		if lastError != nil {
			errMsg = lastError.Error()
		}
		record := &requestlog.RequestLog{
			Status:        requestlog.StatusError,
			CompleteTime:  time.Now(),
			DurationMs:    time.Since(startTime).Milliseconds(),
			Model:         claudeReq.Model,
			HTTPStatus:    httpStatus,
			Error:         errMsg,
			UpstreamError: upstreamErr,
			FailoverInfo:  failoverInfo,
		}
		if lastFailedUpstream != nil {
			record.Type = lastFailedUpstream.ServiceType
			record.ProviderName = lastFailedUpstream.Name
			record.ChannelID = lastFailedUpstream.Index
			record.ChannelName = lastFailedUpstream.Name
		}
		_ = reqLogManager.Update(requestLogID, record)
	}

	// Return error response to client
	if lastFailoverError != nil {
		status := lastFailoverError.Status
		if status == 0 {
			status = 503
		}
		SaveErrorDebugLog(c, cfgManager, reqLogManager, requestLogID, status, lastFailoverError.Body)
		var errBody map[string]interface{}
		if err := json.Unmarshal(lastFailoverError.Body, &errBody); err == nil {
			c.JSON(status, errBody)
		} else {
			c.JSON(status, gin.H{"error": string(lastFailoverError.Body)})
		}
	} else {
		errMsg := "all channels unavailable"
		if lastError != nil {
			errMsg = lastError.Error()
		}
		errJSON := fmt.Sprintf(`{"error":"all channels unavailable","details":"%s"}`, errMsg)
		SaveErrorDebugLog(c, cfgManager, reqLogManager, requestLogID, 503, []byte(errJSON))
		c.JSON(503, gin.H{
			"error":   "all channels unavailable",
			"details": errMsg,
		})
	}
}

// tryChannelWithAllKeys tries all API keys for a channel.
// Returns (success bool, lastFailoverError *struct{Status int; Body []byte; FailoverInfo string}, updatedRequestLogID string)
func tryChannelWithAllKeys(
	c *gin.Context,
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	upstream *config.UpstreamConfig,
	bodyBytes []byte,
	claudeReq types.ClaudeRequest,
	startTime time.Time,
	reqLogManager *requestlog.Manager,
	requestLogID string,
	usageManager *quota.UsageManager,
	failoverTracker *config.FailoverTracker,
	clientID string,
	sessionID string,
	apiKeyID *int64,
) (bool, *struct {
	Status       int
	Body         []byte
	FailoverInfo string
}, string) {
	if len(upstream.APIKeys) == 0 {
		return false, nil, requestLogID
	}

	provider := providers.GetProvider(upstream.ServiceType)
	if provider == nil {
		return false, nil, requestLogID
	}

	maxRetries := len(upstream.APIKeys)
	failedKeys := make(map[string]bool)
	var lastFailoverError *struct {
		Status       int
		Body         []byte
		FailoverInfo string
	}
	deprioritizeCandidates := make(map[string]bool)
	var pinnedKey string      // For retry-same-key scenarios
	var retryWaitPending bool // Allows loop to continue for one retry after wait
	var retryWaitUsed bool    // Tracks if retry_wait already attempted for current key
	currentStartTime := startTime
	currentRequestLogID := requestLogID

	for attempt := 0; attempt < maxRetries || retryWaitPending; {
		retryWaitPending = false // Clear at start of each iteration

		// æ¢å¤è¯·æ±‚ä½“
		c.Request.Body = io.NopCloser(bytes.NewReader(bodyBytes))

		var apiKey string
		var err error

		// If we have a pinned key from a previous retry-same-key decision, use it
		if pinnedKey != "" {
			apiKey = pinnedKey
			pinnedKey = "" // Clear after use
			// Don't increment attempt for retry-same-key
		} else {
			apiKey, err = cfgManager.GetNextAPIKey(upstream, failedKeys)
			if err != nil {
				break
			}
			attempt++           // Only increment when trying a new key
			retryWaitUsed = false // Reset retry_wait flag for new key
		}

		if envCfg.ShouldLog("info") {
			log.Printf("ğŸ”‘ ä½¿ç”¨APIå¯†é’¥: %s (å°è¯• %d/%d)", maskAPIKey(apiKey), attempt+1, maxRetries)
		}

		// è½¬æ¢è¯·æ±‚
		providerReq, _, err := provider.ConvertToProviderRequest(c, upstream, apiKey)
		if err != nil {
			failedKeys[apiKey] = true
			continue
		}

		// å‘é€è¯·æ±‚
		resp, err := sendRequest(providerReq, upstream, envCfg, claudeReq.Stream)
		if err != nil {
			failedKeys[apiKey] = true
			cfgManager.MarkKeyAsFailed(apiKey)
			log.Printf("âš ï¸ APIå¯†é’¥å¤±è´¥: %v", err)
			continue
		}

		// æ£€æŸ¥å“åº”çŠ¶æ€
		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			respBodyBytes, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			respBodyBytes = utils.DecompressGzipIfNeeded(resp, respBodyBytes)

			// Handle 429 errors with smart subtype detection
			if resp.StatusCode == 429 && failoverTracker != nil {
				// Choose failover logic based on quota type
				var decision config.FailoverDecision
				if upstream.QuotaType != "" {
					// Quota channel: use admin failover settings
					failoverConfig := cfgManager.GetFailoverConfig()
					decision = failoverTracker.Decide429Action(upstream.Index, apiKey, respBodyBytes, &failoverConfig)
				} else {
					// Normal channel: use legacy circuit breaker (immediate failover on 429)
					decision = failoverTracker.LegacyFailover(resp.StatusCode)
				}

				switch decision.Action {
				case config.ActionRetrySameKey:
					// Check if we already attempted retry_wait for this key
					if retryWaitUsed {
						// Already retried once, convert to failover
						log.Printf("âš ï¸ 429 %s: retry_wait already used, failing over", decision.Reason)
						failedKeys[apiKey] = true
						lastFailoverError = &struct {
							Status       int
							Body         []byte
							FailoverInfo string
						}{
							Status:       resp.StatusCode,
							Body:         respBodyBytes,
							FailoverInfo: requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionFailover, "retry exhausted"),
						}
						continue
					}

					// Wait and retry with same key
					log.Printf("â³ 429 %s: ç­‰å¾… %v åé‡è¯•åŒä¸€å¯†é’¥", decision.Reason, decision.Wait)

					failoverInfo := requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionRetryWait, fmt.Sprintf("%.0fs", decision.Wait.Seconds()))

					// AUDIT: Log this retry_wait attempt before waiting
					if reqLogManager != nil && currentRequestLogID != "" {
						completeTime := time.Now()
						retryWaitRecord := &requestlog.RequestLog{
							Status:        requestlog.StatusRetryWait,
							CompleteTime:  completeTime,
							DurationMs:    completeTime.Sub(currentStartTime).Milliseconds(),
							Type:          upstream.ServiceType,
							ProviderName:  upstream.Name,
							HTTPStatus:    resp.StatusCode,
							ChannelID:     upstream.Index,
							ChannelName:   upstream.Name,
							Error:         fmt.Sprintf("429 %s - retrying after %v", decision.Reason, decision.Wait),
							UpstreamError: string(respBodyBytes),
							FailoverInfo:  failoverInfo,
						}
						if err := reqLogManager.Update(currentRequestLogID, retryWaitRecord); err != nil {
							log.Printf("âš ï¸ Failed to update retry_wait log: %v", err)
						}

						// Save debug log for this 429 error response
						SaveDebugLog(c, cfgManager, reqLogManager, currentRequestLogID, resp.StatusCode, resp.Header, respBodyBytes)

						// Create new pending log for the retry attempt
						newPendingLog := &requestlog.RequestLog{
							Status:      requestlog.StatusPending,
							InitialTime: time.Now(),
							Model:       claudeReq.Model,
							Stream:      claudeReq.Stream,
							Endpoint:    "/v1/messages",
							ClientID:    clientID,
							SessionID:   sessionID,
							APIKeyID:    apiKeyID,
						}
						if err := reqLogManager.Add(newPendingLog); err != nil {
							log.Printf("âš ï¸ Failed to create retry pending log: %v", err)
						} else {
							currentRequestLogID = newPendingLog.ID
						}
					}

					// Capture for last-resort error reporting
					lastFailoverError = &struct {
						Status       int
						Body         []byte
						FailoverInfo string
					}{
						Status:       resp.StatusCode,
						Body:         respBodyBytes,
						FailoverInfo: failoverInfo,
					}

					select {
					case <-time.After(decision.Wait):
						pinnedKey = apiKey         // Pin for next attempt
						retryWaitUsed = true       // Mark that we've used retry_wait
						retryWaitPending = true    // Allow loop to continue
						currentStartTime = time.Now() // Reset startTime after wait completes
						continue
					case <-c.Request.Context().Done():
						// Client disconnected
						return false, nil, currentRequestLogID
					}

				case config.ActionFailoverKey:
					// Immediate failover to next key
					failedKeys[apiKey] = true
					if decision.MarkKeyFailed {
						cfgManager.MarkKeyAsFailed(apiKey)
					}
					log.Printf("âš ï¸ 429 %s: ç«‹å³åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥", decision.Reason)

					lastFailoverError = &struct {
						Status       int
						Body         []byte
						FailoverInfo string
					}{
						Status:       resp.StatusCode,
						Body:         respBodyBytes,
						FailoverInfo: requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionFailover, "next key"),
					}

					if decision.DeprioritizeKey {
						deprioritizeCandidates[apiKey] = true
					}
					continue

				case config.ActionSuspendChan:
					// Suspend channel until quota resets
					if reqLogManager != nil && decision.SuspendChannel {
						// Calculate suspension duration: use quota reset time if available, default 5 min
						suspendedUntil := time.Now().Add(5 * time.Minute)
						if upstream.QuotaResetAt != nil && upstream.QuotaResetAt.After(time.Now()) {
							suspendedUntil = *upstream.QuotaResetAt
							log.Printf("â¸ï¸ [Messages] Channel [%d] %s: using QuotaResetAt %s for suspension",
								upstream.Index, upstream.Name, suspendedUntil.Format(time.RFC3339))
						} else {
							log.Printf("â¸ï¸ [Messages] Channel [%d] %s: using default 5min suspension (QuotaResetAt: %v)",
								upstream.Index, upstream.Name, upstream.QuotaResetAt)
						}
						channelType := "messages" // Multi-channel proxy is always Messages API
						if err := reqLogManager.SuspendChannel(upstream.Index, channelType, suspendedUntil, decision.Reason); err != nil {
							log.Printf("âš ï¸ Failed to suspend channel [%d] (%s): %v", upstream.Index, channelType, err)
						}
					}
					log.Printf("â¸ï¸ 429 %s: æ¸ é“æš‚åœï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¸ é“", decision.Reason)

					// Return false to trigger channel failover (outer loop will try next channel)
					return false, &struct {
						Status       int
						Body         []byte
						FailoverInfo string
					}{
						Status:       resp.StatusCode,
						Body:         respBodyBytes,
						FailoverInfo: requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionSuspended, "next channel"),
					}, currentRequestLogID

				default:
					// ActionNone - return error to client
					if reqLogManager != nil && currentRequestLogID != "" {
						completeTime := time.Now()
						record := &requestlog.RequestLog{
							Status:        requestlog.StatusError,
							CompleteTime:  completeTime,
							DurationMs:    completeTime.Sub(currentStartTime).Milliseconds(),
							Type:          upstream.ServiceType,
							ProviderName:  upstream.Name,
							HTTPStatus:    resp.StatusCode,
							ChannelID:     upstream.Index,
							ChannelName:   upstream.Name,
							Error:         fmt.Sprintf("429 %s (threshold not reached)", decision.Reason),
							UpstreamError: string(respBodyBytes),
							FailoverInfo:  requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionReturnErr, "threshold not reached"),
						}
						_ = reqLogManager.Update(currentRequestLogID, record)
					}
					SaveDebugLog(c, cfgManager, reqLogManager, currentRequestLogID, resp.StatusCode, resp.Header, respBodyBytes)
					c.Data(resp.StatusCode, "application/json", respBodyBytes)
					return true, nil, currentRequestLogID
				}
			}

			// Non-429 errors: choose failover logic based on quota type
			var shouldFailover, isQuotaRelated bool
			if failoverTracker != nil {
				if upstream.QuotaType != "" {
					// Quota channel: use admin failover settings
					failoverConfig := cfgManager.GetFailoverConfig()
					shouldFailover, isQuotaRelated = failoverTracker.ShouldFailover(upstream.Index, apiKey, resp.StatusCode, &failoverConfig)
				} else {
					// Normal channel: use legacy circuit breaker
					decision := failoverTracker.LegacyFailover(resp.StatusCode)
					shouldFailover = decision.Action == config.ActionFailoverKey
					isQuotaRelated = false
				}
			} else {
				shouldFailover, isQuotaRelated = shouldRetryWithNextKey(resp.StatusCode, respBodyBytes)
			}

			if shouldFailover {
				failedKeys[apiKey] = true
				cfgManager.MarkKeyAsFailed(apiKey)
				log.Printf("âš ï¸ APIå¯†é’¥å¤±è´¥ (çŠ¶æ€: %d)ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥", resp.StatusCode)

				// Determine the reason for failover
				failoverReason := requestlog.FailoverActionFailover
				if resp.StatusCode == 401 || resp.StatusCode == 403 {
					failoverReason = requestlog.FailoverActionAuthFailed
				}
				lastFailoverError = &struct {
					Status       int
					Body         []byte
					FailoverInfo string
				}{
					Status:       resp.StatusCode,
					Body:         respBodyBytes,
					FailoverInfo: requestlog.FormatFailoverInfo(resp.StatusCode, "", failoverReason, "next key"),
				}

				if isQuotaRelated {
					deprioritizeCandidates[apiKey] = true
				}
				continue
			}

			// é failover é”™è¯¯ï¼Œæ›´æ–°è¯·æ±‚æ—¥å¿—å¹¶è¿”å›
			if reqLogManager != nil && currentRequestLogID != "" {
				completeTime := time.Now()
				record := &requestlog.RequestLog{
					Status:        requestlog.StatusError,
					CompleteTime:  completeTime,
					DurationMs:    completeTime.Sub(currentStartTime).Milliseconds(),
					Type:          upstream.ServiceType,
					ProviderName:  upstream.Name,
					HTTPStatus:    resp.StatusCode,
					ChannelID:     upstream.Index,
					ChannelName:   upstream.Name,
					Error:         fmt.Sprintf("upstream returned status %d", resp.StatusCode),
					UpstreamError: string(respBodyBytes),
					FailoverInfo:  requestlog.FormatFailoverInfo(resp.StatusCode, "", requestlog.FailoverActionReturnErr, ""),
				}
				_ = reqLogManager.Update(currentRequestLogID, record)
			}
			SaveDebugLog(c, cfgManager, reqLogManager, currentRequestLogID, resp.StatusCode, resp.Header, respBodyBytes)
			c.Data(resp.StatusCode, "application/json", respBodyBytes)
			return true, nil, currentRequestLogID // è¿”å› true è¡¨ç¤ºè¯·æ±‚å·²å¤„ç†ï¼ˆè™½ç„¶æ˜¯é”™è¯¯å“åº”ï¼‰
		}

		// å¤„ç†æˆåŠŸå“åº” - reset error counters on success
		if failoverTracker != nil {
			failoverTracker.ResetOnSuccess(upstream.Index, apiKey)
		}
		if len(deprioritizeCandidates) > 0 {
			for key := range deprioritizeCandidates {
				_ = cfgManager.DeprioritizeAPIKey(key)
			}
		}

		if claudeReq.Stream {
			handleStreamResponse(c, resp, provider, envCfg, cfgManager, currentStartTime, upstream, reqLogManager, currentRequestLogID, claudeReq.Model, usageManager)
		} else {
			handleNormalResponse(c, resp, provider, envCfg, cfgManager, currentStartTime, upstream, reqLogManager, currentRequestLogID, claudeReq.Model, usageManager)
		}
		return true, nil, currentRequestLogID
	}

	return false, lastFailoverError, currentRequestLogID
}

// handleSingleChannelProxy å¤„ç†å•æ¸ é“ä»£ç†è¯·æ±‚ï¼ˆç°æœ‰é€»è¾‘ï¼‰
func handleSingleChannelProxy(
	c *gin.Context,
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	bodyBytes []byte,
	claudeReq types.ClaudeRequest,
	startTime time.Time,
	reqLogManager *requestlog.Manager,
	requestLogID string,
	usageManager *quota.UsageManager,
	allowedChannels []int,
	failoverTracker *config.FailoverTracker,
	clientID string,
	sessionID string,
	apiKeyID *int64,
) {
	// è·å–å½“å‰ä¸Šæ¸¸é…ç½®
	upstream, err := cfgManager.GetCurrentUpstream()
	if err != nil {
		c.JSON(503, gin.H{
			"error": "æœªé…ç½®ä»»ä½•æ¸ é“ï¼Œè¯·å…ˆåœ¨ç®¡ç†ç•Œé¢æ·»åŠ æ¸ é“",
			"code":  "NO_UPSTREAM",
		})
		return
	}

	// Check if this channel is allowed by API key permissions
	if len(allowedChannels) > 0 {
		allowed := false
		for _, idx := range allowedChannels {
			if idx == upstream.Index {
				allowed = true
				break
			}
		}
		if !allowed {
			c.JSON(403, gin.H{
				"error": fmt.Sprintf("Channel %s (index %d) not allowed for this API key", upstream.Name, upstream.Index),
				"code":  "CHANNEL_NOT_ALLOWED",
			})
			return
		}
	}

	if len(upstream.APIKeys) == 0 {
		c.JSON(503, gin.H{
			"error": fmt.Sprintf("å½“å‰æ¸ é“ \"%s\" æœªé…ç½®APIå¯†é’¥", upstream.Name),
			"code":  "NO_API_KEYS",
		})
		return
	}

	// è·å–æä¾›å•†
	provider := providers.GetProvider(upstream.ServiceType)
	if provider == nil {
		c.JSON(400, gin.H{"error": "Unsupported service type"})
		return
	}

	// å®ç° failover é‡è¯•é€»è¾‘
	maxRetries := len(upstream.APIKeys)
	failedKeys := make(map[string]bool)
	var lastError error
	var lastOriginalBodyBytes []byte
	var lastFailoverError *struct {
		Status       int
		Body         []byte
		FailoverInfo string
	}
	deprioritizeCandidates := make(map[string]bool)
	var pinnedKey string      // For retry-same-key scenarios
	var retryWaitPending bool // Allows loop to continue for one retry after wait
	var retryWaitUsed bool    // Tracks if retry_wait already attempted for current key
	currentStartTime := startTime
	currentRequestLogID := requestLogID

	for attempt := 0; attempt < maxRetries || retryWaitPending; {
		retryWaitPending = false // Clear at start of each iteration

		// æ¢å¤è¯·æ±‚ä½“
		c.Request.Body = io.NopCloser(bytes.NewReader(bodyBytes))

		var apiKey string
		var err error

		// If we have a pinned key from a previous retry-same-key decision, use it
		if pinnedKey != "" {
			apiKey = pinnedKey
			pinnedKey = "" // Clear after use
			// Don't increment attempt for retry-same-key
		} else {
			apiKey, err = cfgManager.GetNextAPIKey(upstream, failedKeys)
			if err != nil {
				lastError = err
				break
			}
			attempt++             // Only increment when trying a new key
			retryWaitUsed = false // Reset retry_wait flag for new key
		}

		if envCfg.ShouldLog("info") {
			log.Printf("ğŸ¯ ä½¿ç”¨ä¸Šæ¸¸: %s - %s (å°è¯• %d/%d)", upstream.Name, upstream.BaseURL, attempt+1, maxRetries)
			log.Printf("ğŸ”‘ ä½¿ç”¨APIå¯†é’¥: %s", maskAPIKey(apiKey))
		}

		// è½¬æ¢è¯·æ±‚
		providerReq, originalBodyBytes, err := provider.ConvertToProviderRequest(c, upstream, apiKey)
		if err != nil {
			lastError = err
			failedKeys[apiKey] = true
			if originalBodyBytes != nil {
				lastOriginalBodyBytes = originalBodyBytes
			}
			continue
		}
		lastOriginalBodyBytes = originalBodyBytes

		// è¯·æ±‚æ—¥å¿—è®°å½•
		if envCfg.EnableRequestLogs {
			log.Printf("ğŸ“¥ æ”¶åˆ°è¯·æ±‚: %s %s", c.Request.Method, c.Request.URL.Path)
			if envCfg.IsDevelopment() {
				logBody := lastOriginalBodyBytes
				if len(logBody) == 0 && c.Request.Body != nil {
					bodyFromContext, _ := io.ReadAll(c.Request.Body)
					c.Request.Body = io.NopCloser(bytes.NewReader(bodyFromContext))
					logBody = bodyFromContext
				}
				formattedBody := utils.FormatJSONBytesForLog(logBody, 500)
				log.Printf("ğŸ“„ åŸå§‹è¯·æ±‚ä½“:\n%s", formattedBody)

				sanitizedHeaders := make(map[string]string)
				for key, values := range c.Request.Header {
					if len(values) > 0 {
						sanitizedHeaders[key] = values[0]
					}
				}
				maskedHeaders := utils.MaskSensitiveHeaders(sanitizedHeaders)
				headersJSON, _ := json.MarshalIndent(maskedHeaders, "", "  ")
				log.Printf("ğŸ“¥ åŸå§‹è¯·æ±‚å¤´:\n%s", string(headersJSON))
			}
		}

		// å‘é€è¯·æ±‚
		resp, err := sendRequest(providerReq, upstream, envCfg, claudeReq.Stream)
		if err != nil {
			lastError = err
			failedKeys[apiKey] = true
			cfgManager.MarkKeyAsFailed(apiKey)
			log.Printf("âš ï¸ APIå¯†é’¥å¤±è´¥: %v", err)
			continue
		}

		// æ£€æŸ¥å“åº”çŠ¶æ€
		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			respBodyBytes, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			respBodyBytes = utils.DecompressGzipIfNeeded(resp, respBodyBytes)

			// Handle 429 errors with smart subtype detection
			if resp.StatusCode == 429 && failoverTracker != nil {
				// Choose failover logic based on quota type
				var decision config.FailoverDecision
				if upstream.QuotaType != "" {
					// Quota channel: use admin failover settings
					failoverConfig := cfgManager.GetFailoverConfig()
					decision = failoverTracker.Decide429Action(upstream.Index, apiKey, respBodyBytes, &failoverConfig)
				} else {
					// Normal channel: use legacy circuit breaker (immediate failover on 429)
					decision = failoverTracker.LegacyFailover(resp.StatusCode)
				}

				switch decision.Action {
				case config.ActionRetrySameKey:
					// Check if we already attempted retry_wait for this key
					if retryWaitUsed {
						// Already retried once, convert to failover
						log.Printf("âš ï¸ 429 %s: retry_wait already used, failing over", decision.Reason)
						lastError = fmt.Errorf("429 %s: retry exhausted", decision.Reason)
						failedKeys[apiKey] = true
						lastFailoverError = &struct {
							Status       int
							Body         []byte
							FailoverInfo string
						}{
							Status:       resp.StatusCode,
							Body:         respBodyBytes,
							FailoverInfo: requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionFailover, "retry exhausted"),
						}
						continue
					}

					// Wait and retry with same key
					log.Printf("â³ 429 %s: ç­‰å¾… %v åé‡è¯•åŒä¸€å¯†é’¥", decision.Reason, decision.Wait)

					failoverInfo := requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionRetryWait, fmt.Sprintf("%.0fs", decision.Wait.Seconds()))

					// AUDIT: Log this retry_wait attempt before waiting
					if reqLogManager != nil && currentRequestLogID != "" {
						completeTime := time.Now()
						retryWaitRecord := &requestlog.RequestLog{
							Status:        requestlog.StatusRetryWait,
							CompleteTime:  completeTime,
							DurationMs:    completeTime.Sub(currentStartTime).Milliseconds(),
							Type:          upstream.ServiceType,
							ProviderName:  upstream.Name,
							HTTPStatus:    resp.StatusCode,
							ChannelID:     upstream.Index,
							ChannelName:   upstream.Name,
							Error:         fmt.Sprintf("429 %s - retrying after %v", decision.Reason, decision.Wait),
							UpstreamError: string(respBodyBytes),
							FailoverInfo:  failoverInfo,
						}
						if err := reqLogManager.Update(currentRequestLogID, retryWaitRecord); err != nil {
							log.Printf("âš ï¸ Failed to update retry_wait log: %v", err)
						}

						// Save debug log for this 429 error response
						SaveDebugLog(c, cfgManager, reqLogManager, currentRequestLogID, resp.StatusCode, resp.Header, respBodyBytes)

						// Create new pending log for the retry attempt
						newPendingLog := &requestlog.RequestLog{
							Status:      requestlog.StatusPending,
							InitialTime: time.Now(),
							Model:       claudeReq.Model,
							Stream:      claudeReq.Stream,
							Endpoint:    "/v1/messages",
							ClientID:    clientID,
							SessionID:   sessionID,
							APIKeyID:    apiKeyID,
						}
						if err := reqLogManager.Add(newPendingLog); err != nil {
							log.Printf("âš ï¸ Failed to create retry pending log: %v", err)
						} else {
							currentRequestLogID = newPendingLog.ID
						}
					}

					select {
					case <-time.After(decision.Wait):
						pinnedKey = apiKey         // Pin for next attempt
						retryWaitUsed = true       // Mark that we've used retry_wait
						retryWaitPending = true    // Allow loop to continue
						currentStartTime = time.Now() // Reset startTime after wait completes
						continue
					case <-c.Request.Context().Done():
						// Client disconnected
						return
					}

				case config.ActionFailoverKey:
					// Immediate failover to next key
					lastError = fmt.Errorf("429 %s", decision.Reason)
					failedKeys[apiKey] = true
					if decision.MarkKeyFailed {
						cfgManager.MarkKeyAsFailed(apiKey)
					}
					log.Printf("âš ï¸ 429 %s: ç«‹å³åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥", decision.Reason)
					if envCfg.EnableResponseLogs && envCfg.IsDevelopment() {
						formattedBody := utils.FormatJSONBytesForLog(respBodyBytes, 500)
						log.Printf("ğŸ“¦ å¤±è´¥åŸå› :\n%s", formattedBody)
					}

					lastFailoverError = &struct {
						Status       int
						Body         []byte
						FailoverInfo string
					}{
						Status:       resp.StatusCode,
						Body:         respBodyBytes,
						FailoverInfo: requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionFailover, "next key"),
					}

					if decision.DeprioritizeKey {
						deprioritizeCandidates[apiKey] = true
					}
					continue

				case config.ActionSuspendChan:
					// Suspend channel until quota resets (single-channel mode)
					// Record suspension for monitoring, but return error to client since no fallback
					if reqLogManager != nil && decision.SuspendChannel {
						suspendedUntil := time.Now().Add(5 * time.Minute)
						if upstream.QuotaResetAt != nil && upstream.QuotaResetAt.After(time.Now()) {
							suspendedUntil = *upstream.QuotaResetAt
						}
						if err := reqLogManager.SuspendChannel(upstream.Index, "messages", suspendedUntil, decision.Reason); err != nil {
							log.Printf("âš ï¸ Failed to suspend channel [%d] (messages): %v", upstream.Index, err)
						}
					}
					log.Printf("â¸ï¸ 429 %s: æ¸ é“æš‚åœ (å•æ¸ é“æ¨¡å¼ï¼Œæ— å¯ç”¨åå¤‡)", decision.Reason)

					// Update request log and return error to client
					if reqLogManager != nil && currentRequestLogID != "" {
						completeTime := time.Now()
						record := &requestlog.RequestLog{
							Status:        requestlog.StatusError,
							CompleteTime:  completeTime,
							DurationMs:    completeTime.Sub(currentStartTime).Milliseconds(),
							Type:          upstream.ServiceType,
							ProviderName:  upstream.Name,
							HTTPStatus:    resp.StatusCode,
							ChannelID:     upstream.Index,
							ChannelName:   upstream.Name,
							Error:         fmt.Sprintf("429 %s (channel suspended)", decision.Reason),
							UpstreamError: string(respBodyBytes),
							FailoverInfo:  requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionSuspended, "no fallback"),
						}
						_ = reqLogManager.Update(currentRequestLogID, record)
					}
					SaveDebugLog(c, cfgManager, reqLogManager, currentRequestLogID, resp.StatusCode, resp.Header, respBodyBytes)
					c.Data(resp.StatusCode, "application/json", respBodyBytes)
					return

				default:
					// ActionNone - return error to client
					if envCfg.EnableResponseLogs {
						log.Printf("âš ï¸ 429 %s (threshold not reached)", decision.Reason)
					}
					if reqLogManager != nil && currentRequestLogID != "" {
						completeTime := time.Now()
						record := &requestlog.RequestLog{
							Status:        requestlog.StatusError,
							CompleteTime:  completeTime,
							DurationMs:    completeTime.Sub(currentStartTime).Milliseconds(),
							Type:          upstream.ServiceType,
							ProviderName:  upstream.Name,
							HTTPStatus:    resp.StatusCode,
							ChannelID:     upstream.Index,
							ChannelName:   upstream.Name,
							Error:         fmt.Sprintf("429 %s (threshold not reached)", decision.Reason),
							UpstreamError: string(respBodyBytes),
							FailoverInfo:  requestlog.FormatFailoverInfo(resp.StatusCode, decision.Reason, requestlog.FailoverActionReturnErr, "threshold not reached"),
						}
						_ = reqLogManager.Update(currentRequestLogID, record)
					}
					SaveDebugLog(c, cfgManager, reqLogManager, currentRequestLogID, resp.StatusCode, resp.Header, respBodyBytes)
					c.Data(resp.StatusCode, "application/json", respBodyBytes)
					return
				}
			}

			// Non-429 errors: choose failover logic based on quota type
			var shouldFailover, isQuotaRelated bool
			if failoverTracker != nil {
				if upstream.QuotaType != "" {
					// Quota channel: use admin failover settings
					failoverConfig := cfgManager.GetFailoverConfig()
					shouldFailover, isQuotaRelated = failoverTracker.ShouldFailover(upstream.Index, apiKey, resp.StatusCode, &failoverConfig)
				} else {
					// Normal channel: use legacy circuit breaker
					decision := failoverTracker.LegacyFailover(resp.StatusCode)
					shouldFailover = decision.Action == config.ActionFailoverKey
					isQuotaRelated = false
				}
			} else {
				shouldFailover, isQuotaRelated = shouldRetryWithNextKey(resp.StatusCode, respBodyBytes)
			}

			if shouldFailover {
				lastError = fmt.Errorf("ä¸Šæ¸¸é”™è¯¯: %d", resp.StatusCode)
				failedKeys[apiKey] = true
				cfgManager.MarkKeyAsFailed(apiKey)

				log.Printf("âš ï¸ APIå¯†é’¥å¤±è´¥ (çŠ¶æ€: %d)ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥", resp.StatusCode)
				if envCfg.EnableResponseLogs && envCfg.IsDevelopment() {
					formattedBody := utils.FormatJSONBytesForLog(respBodyBytes, 500)
					log.Printf("ğŸ“¦ å¤±è´¥åŸå› :\n%s", formattedBody)
				} else if envCfg.EnableResponseLogs {
					log.Printf("å¤±è´¥åŸå› : %s", string(respBodyBytes))
				}

				lastFailoverError = &struct {
					Status       int
					Body         []byte
					FailoverInfo string
				}{
					Status:       resp.StatusCode,
					Body:         respBodyBytes,
					FailoverInfo: requestlog.FormatFailoverInfo(resp.StatusCode, "", requestlog.FailoverActionFailover, "next key"),
				}

				if isQuotaRelated {
					deprioritizeCandidates[apiKey] = true
				}
				continue
			}

			// é failover é”™è¯¯
			if envCfg.EnableResponseLogs {
				log.Printf("âš ï¸ ä¸Šæ¸¸è¿”å›é”™è¯¯: %d", resp.StatusCode)
				if envCfg.IsDevelopment() {
					formattedBody := utils.FormatJSONBytesForLog(respBodyBytes, 500)
					log.Printf("ğŸ“¦ é”™è¯¯å“åº”ä½“:\n%s", formattedBody)

					respHeaders := make(map[string]string)
					for key, values := range resp.Header {
						if len(values) > 0 {
							respHeaders[key] = values[0]
						}
					}
					respHeadersJSON, _ := json.MarshalIndent(respHeaders, "", "  ")
					log.Printf("ğŸ“‹ é”™è¯¯å“åº”å¤´:\n%s", string(respHeadersJSON))
				}
				}
				// æ›´æ–°è¯·æ±‚æ—¥å¿—ä¸ºé”™è¯¯çŠ¶æ€ï¼ˆé failover é”™è¯¯ä¹Ÿéœ€è¦ç»“æŸ pendingï¼‰
				if reqLogManager != nil && currentRequestLogID != "" {
					completeTime := time.Now()
					record := &requestlog.RequestLog{
						Status:        requestlog.StatusError,
						CompleteTime:  completeTime,
						DurationMs:    completeTime.Sub(currentStartTime).Milliseconds(),
						Type:          upstream.ServiceType,
						ProviderName:  upstream.Name,
						HTTPStatus:    resp.StatusCode,
						ChannelID:     upstream.Index,
						ChannelName:   upstream.Name,
						Error:         fmt.Sprintf("upstream returned status %d", resp.StatusCode),
						UpstreamError: string(respBodyBytes),
						FailoverInfo:  requestlog.FormatFailoverInfo(resp.StatusCode, "", requestlog.FailoverActionReturnErr, ""),
					}
					_ = reqLogManager.Update(currentRequestLogID, record)
				}
				SaveDebugLog(c, cfgManager, reqLogManager, currentRequestLogID, resp.StatusCode, resp.Header, respBodyBytes)
				c.Data(resp.StatusCode, "application/json", respBodyBytes)
				return
			}

		// å¤„ç†æˆåŠŸå“åº” - reset error counters on success
		if failoverTracker != nil {
			failoverTracker.ResetOnSuccess(upstream.Index, apiKey)
		}
		if len(deprioritizeCandidates) > 0 {
			for key := range deprioritizeCandidates {
				if err := cfgManager.DeprioritizeAPIKey(key); err != nil {
					log.Printf("âš ï¸ å¯†é’¥é™çº§å¤±è´¥: %v", err)
				}
			}
		}

		if claudeReq.Stream {
			handleStreamResponse(c, resp, provider, envCfg, cfgManager, currentStartTime, upstream, reqLogManager, currentRequestLogID, claudeReq.Model, usageManager)
		} else {
			handleNormalResponse(c, resp, provider, envCfg, cfgManager, currentStartTime, upstream, reqLogManager, currentRequestLogID, claudeReq.Model, usageManager)
		}
		return
	}

	// æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥äº†
	log.Printf("ğŸ’¥ æ‰€æœ‰APIå¯†é’¥éƒ½å¤±è´¥äº†")

	// æ›´æ–°è¯·æ±‚æ—¥å¿—ä¸ºé”™è¯¯çŠ¶æ€
	if reqLogManager != nil && currentRequestLogID != "" {
		httpStatus := 500
		errMsg := "æ‰€æœ‰APIå¯†é’¥éƒ½ä¸å¯ç”¨"
		upstreamErr := ""
		failoverInfo := ""
		if lastFailoverError != nil && lastFailoverError.Status != 0 {
			httpStatus = lastFailoverError.Status
			upstreamErr = string(lastFailoverError.Body)
			failoverInfo = lastFailoverError.FailoverInfo
		}
		if lastError != nil {
			errMsg = lastError.Error()
		}
		record := &requestlog.RequestLog{
			Status:        requestlog.StatusError,
			CompleteTime:  time.Now(),
			DurationMs:    time.Since(currentStartTime).Milliseconds(),
			Model:         claudeReq.Model,
			Type:          upstream.ServiceType,
			ProviderName:  upstream.Name,
			HTTPStatus:    httpStatus,
			Error:         errMsg,
			UpstreamError: upstreamErr,
			FailoverInfo:  failoverInfo,
		}
		_ = reqLogManager.Update(currentRequestLogID, record)
	}

	if lastFailoverError != nil {
		status := lastFailoverError.Status
		if status == 0 {
			status = 500
		}
		SaveErrorDebugLog(c, cfgManager, reqLogManager, currentRequestLogID, status, lastFailoverError.Body)
		var errBody map[string]interface{}
		if err := json.Unmarshal(lastFailoverError.Body, &errBody); err == nil {
			c.JSON(status, errBody)
		} else {
			c.JSON(status, gin.H{"error": string(lastFailoverError.Body)})
		}
	} else {
		errMsg := "æœªçŸ¥é”™è¯¯"
		if lastError != nil {
			errMsg = lastError.Error()
		}
		errJSON := fmt.Sprintf(`{"error":"æ‰€æœ‰ä¸Šæ¸¸APIå¯†é’¥éƒ½ä¸å¯ç”¨","details":"%s"}`, errMsg)
		SaveErrorDebugLog(c, cfgManager, reqLogManager, currentRequestLogID, 500, []byte(errJSON))
		c.JSON(500, gin.H{
			"error":   "æ‰€æœ‰ä¸Šæ¸¸APIå¯†é’¥éƒ½ä¸å¯ç”¨",
			"details": errMsg,
		})
	}
}

// sendRequest å‘é€HTTPè¯·æ±‚
func sendRequest(req *http.Request, upstream *config.UpstreamConfig, envCfg *config.EnvConfig, isStream bool) (*http.Response, error) {
	// ä½¿ç”¨å…¨å±€å®¢æˆ·ç«¯ç®¡ç†å™¨
	clientManager := httpclient.GetManager()

	var client *http.Client
	if isStream {
		// æµå¼è¯·æ±‚ï¼šä½¿ç”¨æ— è¶…æ—¶çš„å®¢æˆ·ç«¯ï¼Œä½†æœ‰å“åº”å¤´è¶…æ—¶
		client = clientManager.GetStreamClient(upstream.InsecureSkipVerify, upstream.GetResponseHeaderTimeout())
	} else {
		// æ™®é€šè¯·æ±‚ï¼šä½¿ç”¨æœ‰è¶…æ—¶çš„å®¢æˆ·ç«¯ï¼ŒåŒæ—¶åº”ç”¨æ¸ é“çš„å“åº”å¤´è¶…æ—¶è®¾ç½®
		timeout := time.Duration(envCfg.RequestTimeout) * time.Millisecond
		client = clientManager.GetStandardClient(timeout, upstream.InsecureSkipVerify, upstream.GetResponseHeaderTimeout())
	}

	if upstream.InsecureSkipVerify && envCfg.EnableRequestLogs {
		log.Printf("âš ï¸ æ­£åœ¨è·³è¿‡å¯¹ %s çš„TLSè¯ä¹¦éªŒè¯", req.URL.String())
	}

	if envCfg.EnableRequestLogs {
		log.Printf("ğŸŒ å®é™…è¯·æ±‚URL: %s", req.URL.String())
		log.Printf("ğŸ“¤ è¯·æ±‚æ–¹æ³•: %s", req.Method)
		if envCfg.IsDevelopment() {
			// å¯¹è¯·æ±‚å¤´åšæ•æ„Ÿä¿¡æ¯è„±æ•
			reqHeaders := make(map[string]string)
			for key, values := range req.Header {
				if len(values) > 0 {
					reqHeaders[key] = values[0]
				}
			}
			maskedReqHeaders := utils.MaskSensitiveHeaders(reqHeaders)
			reqHeadersJSON, _ := json.MarshalIndent(maskedReqHeaders, "", "  ")
			log.Printf("ğŸ“‹ å®é™…è¯·æ±‚å¤´:\n%s", string(reqHeadersJSON))

			if req.Body != nil {
				// è¯»å–è¯·æ±‚ä½“ç”¨äºæ—¥å¿—
				bodyBytes, err := io.ReadAll(req.Body)
				if err == nil {
					// æ¢å¤è¯·æ±‚ä½“
					req.Body = io.NopCloser(bytes.NewReader(bodyBytes))

					// ä½¿ç”¨æ™ºèƒ½æˆªæ–­å’Œç®€åŒ–å‡½æ•°ï¼ˆä¸TSç‰ˆæœ¬å¯¹é½ï¼‰
					formattedBody := utils.FormatJSONBytesForLog(bodyBytes, 500)
					log.Printf("ğŸ“¦ å®é™…è¯·æ±‚ä½“:\n%s", formattedBody)
				}
			}
		}
	}

	return client.Do(req)
}

// trackMessagesUsage tracks usage for Messages API channels based on quota type
func trackMessagesUsage(usageManager *quota.UsageManager, upstream *config.UpstreamConfig, model string, cost float64) {
	if usageManager == nil || upstream.QuotaType == "" {
		return
	}

	// Check if this model should be counted for quota
	if !upstream.ShouldCountQuota(model) {
		return
	}

	var amount float64
	switch upstream.QuotaType {
	case "requests":
		amount = 1
	case "credit":
		amount = cost
	default:
		return
	}

	if err := usageManager.IncrementUsage(upstream.Index, amount); err != nil {
		log.Printf("âš ï¸ é…é¢ä½¿ç”¨é‡è¿½è¸ªå¤±è´¥ (Messages, channelIndex=%d): %v", upstream.Index, err)
	}
}

// handleNormalResponse å¤„ç†éæµå¼å“åº”
func handleNormalResponse(c *gin.Context, resp *http.Response, provider providers.Provider, envCfg *config.EnvConfig, cfgManager *config.ConfigManager, startTime time.Time, upstream *config.UpstreamConfig, reqLogManager *requestlog.Manager, requestLogID string, requestModel string, usageManager *quota.UsageManager) {
	defer resp.Body.Close()

	bodyBytes, err := io.ReadAll(resp.Body)
	if err != nil {
		c.JSON(500, gin.H{"error": "Failed to read response"})
		return
	}

	completeTime := time.Now()
	durationMs := completeTime.Sub(startTime).Milliseconds()

	if envCfg.EnableResponseLogs {
		log.Printf("â±ï¸ å“åº”å®Œæˆ: %dms, çŠ¶æ€: %d", durationMs, resp.StatusCode)
		if envCfg.IsDevelopment() {
			// å“åº”å¤´(ä¸éœ€è¦è„±æ•)
			respHeaders := make(map[string]string)
			for key, values := range resp.Header {
				if len(values) > 0 {
					respHeaders[key] = values[0]
				}
			}
			respHeadersJSON, _ := json.MarshalIndent(respHeaders, "", "  ")
			log.Printf("ğŸ“‹ å“åº”å¤´:\n%s", string(respHeadersJSON))

			// ä½¿ç”¨æ™ºèƒ½æˆªæ–­ï¼ˆä¸TSç‰ˆæœ¬å¯¹é½ï¼‰
			formattedBody := utils.FormatJSONBytesForLog(bodyBytes, 500)
			log.Printf("ğŸ“¦ å“åº”ä½“:\n%s", formattedBody)
		}
	}

	providerResp := &types.ProviderResponse{
		StatusCode: resp.StatusCode,
		Headers:    resp.Header,
		Body:       bodyBytes,
		Stream:     false,
	}

	claudeResp, err := provider.ConvertToClaudeResponse(providerResp)
	if err != nil {
		c.JSON(500, gin.H{"error": "Failed to convert response"})
		return
	}

	// ç›‘å¬å“åº”å…³é—­äº‹ä»¶(å®¢æˆ·ç«¯æ–­å¼€è¿æ¥)
	closeNotify := c.Writer.CloseNotify()
	go func() {
		select {
		case <-closeNotify:
			// æ£€æŸ¥å“åº”æ˜¯å¦å·²å®Œæˆ
			if !c.Writer.Written() {
				if envCfg.EnableResponseLogs {
					responseTime := time.Since(startTime).Milliseconds()
					log.Printf("â±ï¸ å“åº”ä¸­æ–­: %dms, çŠ¶æ€: %d", responseTime, resp.StatusCode)
				}
			}
		case <-time.After(10 * time.Second):
			// è¶…æ—¶é€€å‡ºgoroutine,é¿å…æ³„æ¼
			return
		}
	}()

	// è½¬å‘ä¸Šæ¸¸å“åº”å¤´åˆ°å®¢æˆ·ç«¯ï¼ˆé€æ˜ä»£ç†ï¼‰
	utils.ForwardResponseHeaders(resp.Header, c.Writer)

	c.JSON(200, claudeResp)

	// å“åº”å®Œæˆåè®°å½•
	if envCfg.EnableResponseLogs {
		responseTime := time.Since(startTime).Milliseconds()
		log.Printf("â±ï¸ å“åº”å‘é€å®Œæˆ: %dms, çŠ¶æ€: %d", responseTime, resp.StatusCode)
	}

		// æ›´æ–°è¯·æ±‚æ—¥å¿—ï¼ˆæ‰€æœ‰ä¸Šæ¸¸éƒ½æ›´æ–°ï¼›usage/æˆæœ¬ä»…åœ¨å¯æå–æ—¶å¡«å……ï¼‰
		if reqLogManager != nil && requestLogID != "" {
			var usage *types.Usage
			var responseModel string

			if claudeResp != nil {
				usage = claudeResp.Usage
			}

			// ä»å“åº”ä¸­æå–å®é™…ä½¿ç”¨çš„æ¨¡å‹åï¼ˆè‹¥æœ‰ï¼‰
			var respMap map[string]interface{}
			if err := json.Unmarshal(bodyBytes, &respMap); err == nil {
				if m, ok := respMap["model"].(string); ok {
					responseModel = m
				}
			}

			// ç”¨äºå®šä»·è®¡ç®—çš„æ¨¡å‹åï¼ˆä¼˜å…ˆå“åº”æ¨¡å‹ï¼Œè‹¥æ— å®šä»·é…ç½®åˆ™å›é€€åˆ°è¯·æ±‚æ¨¡å‹ï¼‰
			pricingModel := responseModel
			if pricingModel == "" {
				pricingModel = requestModel
			} else if pm := pricing.GetManager(); pm != nil && !pm.HasPricing(pricingModel) && requestModel != "" {
				pricingModel = requestModel
			}

			record := &requestlog.RequestLog{
				Status:        requestlog.StatusCompleted,
				CompleteTime:  completeTime,
				DurationMs:    durationMs,
				Type:          upstream.ServiceType,
				ProviderName:  upstream.Name,
				ResponseModel: responseModel,
				HTTPStatus:    resp.StatusCode,
				ChannelID:     upstream.Index,
				ChannelName:   upstream.Name,
			}

			if usage != nil {
				record.InputTokens = usage.InputTokens
				record.OutputTokens = usage.OutputTokens
				record.CacheCreationInputTokens = usage.CacheCreationInputTokens
				record.CacheReadInputTokens = usage.CacheReadInputTokens

				if pm := pricing.GetManager(); pm != nil {
					var multipliers *pricing.PriceMultipliers
					if channelMult := upstream.GetPriceMultipliers(pricingModel); channelMult != nil {
						multipliers = &pricing.PriceMultipliers{
							InputMultiplier:         channelMult.GetEffectiveMultiplier("input"),
							OutputMultiplier:        channelMult.GetEffectiveMultiplier("output"),
							CacheCreationMultiplier: channelMult.GetEffectiveMultiplier("cacheCreation"),
							CacheReadMultiplier:     channelMult.GetEffectiveMultiplier("cacheRead"),
						}
					}
					breakdown := pm.CalculateCostWithBreakdown(
						pricingModel,
						usage.InputTokens,
						usage.OutputTokens,
						usage.CacheCreationInputTokens,
						usage.CacheReadInputTokens,
						multipliers,
					)
					record.Price = breakdown.TotalCost
					record.InputCost = breakdown.InputCost
					record.OutputCost = breakdown.OutputCost
					record.CacheCreationCost = breakdown.CacheCreationCost
					record.CacheReadCost = breakdown.CacheReadCost
				}
			}

			if err := reqLogManager.Update(requestLogID, record); err != nil {
				log.Printf("âš ï¸ è¯·æ±‚æ—¥å¿—æ›´æ–°å¤±è´¥: %v", err)
			}

			// Save debug log if enabled
			SaveDebugLog(c, cfgManager, reqLogManager, requestLogID, resp.StatusCode, resp.Header, bodyBytes)

			// Track usage for quota (count 2xx and 400 as successful - 400 is client error but still counts as a request)
			if (resp.StatusCode >= 200 && resp.StatusCode < 300) || resp.StatusCode == 400 {
				trackMessagesUsage(usageManager, upstream, requestModel, record.Price)
			}
		}
	}

// handleStreamResponse å¤„ç†æµå¼å“åº”
func handleStreamResponse(c *gin.Context, resp *http.Response, provider providers.Provider, envCfg *config.EnvConfig, cfgManager *config.ConfigManager, startTime time.Time, upstream *config.UpstreamConfig, reqLogManager *requestlog.Manager, requestLogID string, requestModel string, usageManager *quota.UsageManager) {
	defer resp.Body.Close()

	eventChan, errChan, err := provider.HandleStreamResponse(resp.Body)
	if err != nil {
		c.JSON(500, gin.H{"error": "Failed to handle stream response"})
		return
	}

	// å…ˆè½¬å‘ä¸Šæ¸¸å“åº”å¤´ï¼ˆé€æ˜ä»£ç†ï¼‰
	utils.ForwardResponseHeaders(resp.Header, c.Writer)

	// è®¾ç½® SSE å“åº”å¤´ï¼ˆå¯èƒ½è¦†ç›–ä¸Šæ¸¸çš„ Content-Typeï¼‰
	c.Header("Content-Type", "text/event-stream")
	c.Header("Cache-Control", "no-cache")
	c.Header("Connection", "keep-alive")
	c.Header("X-Accel-Buffering", "no")

	c.Status(200)

	var logBuffer bytes.Buffer
	var synthesizer *utils.StreamSynthesizer

	// For Claude-style SSE (claude + openai_chat), we need synthesizer to extract usage for request logs.
	needsSynthesizer := (upstream.ServiceType == "claude" || upstream.ServiceType == "openai_chat") && reqLogManager != nil
	streamLoggingEnabled := envCfg.IsDevelopment() && envCfg.EnableResponseLogs

	// Check if debug logging is enabled (need to capture response body)
	debugLogEnabled := cfgManager.GetDebugLogConfig().Enabled

	if streamLoggingEnabled || needsSynthesizer {
		synthesizer = utils.NewStreamSynthesizer(upstream.ServiceType)
	}

	w := c.Writer
	flusher, ok := w.(http.Flusher)
	if !ok {
		log.Printf("âš ï¸ ResponseWriterä¸æ”¯æŒFlushæ¥å£")
		return
	}
	flusher.Flush()

	clientGone := false
	for {
		select {
		case event, ok := <-eventChan:
			if !ok {
				// é€šé“å…³é—­ï¼Œæµå¼ä¼ è¾“ç»“æŸ
				completeTime := time.Now()
				durationMs := completeTime.Sub(startTime).Milliseconds()

				if envCfg.EnableResponseLogs {
					log.Printf("â±ï¸ æµå¼å“åº”å®Œæˆ: %dms", durationMs)

					// æ‰“å°å®Œæ•´çš„å“åº”å†…å®¹
					if envCfg.IsDevelopment() {
						if synthesizer != nil {
							synthesizedContent := synthesizer.GetSynthesizedContent()
							parseFailed := synthesizer.IsParseFailed()
							if synthesizedContent != "" && !parseFailed {
								log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åˆæˆå†…å®¹:\n%s", strings.TrimSpace(synthesizedContent))
							} else if logBuffer.Len() > 0 {
								log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åŸå§‹å†…å®¹:\n%s", logBuffer.String())
							}
						} else if logBuffer.Len() > 0 {
							// synthesizerä¸ºnilæ—¶ï¼Œç›´æ¥æ‰“å°åŸå§‹å†…å®¹
							log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åŸå§‹å†…å®¹:\n%s", logBuffer.String())
						}
					}
				}

					// æ›´æ–°è¯·æ±‚æ—¥å¿—ï¼ˆæ‰€æœ‰ä¸Šæ¸¸éƒ½æ›´æ–°ï¼›usage/æˆæœ¬ä»…åœ¨å¯æå–æ—¶å¡«å……ï¼‰
					if reqLogManager != nil && requestLogID != "" {
						var usage *utils.StreamUsage
						responseModel := ""

						if synthesizer != nil {
							usage = synthesizer.GetUsage()
							responseModel = synthesizer.GetModel()
						}

						pricingModel := responseModel
						if pricingModel == "" {
							pricingModel = requestModel
						} else if pm := pricing.GetManager(); pm != nil && !pm.HasPricing(pricingModel) && requestModel != "" {
							pricingModel = requestModel
						}

						record := &requestlog.RequestLog{
							Status:        requestlog.StatusCompleted,
							CompleteTime:  completeTime,
							DurationMs:    durationMs,
							Type:          upstream.ServiceType,
							ProviderName:  upstream.Name,
							ResponseModel: responseModel,
							HTTPStatus:    resp.StatusCode,
							ChannelID:     upstream.Index,
							ChannelName:   upstream.Name,
						}

						if usage != nil {
							record.InputTokens = usage.InputTokens
							record.OutputTokens = usage.OutputTokens
							record.CacheCreationInputTokens = usage.CacheCreationInputTokens
							record.CacheReadInputTokens = usage.CacheReadInputTokens

							if pm := pricing.GetManager(); pm != nil {
								var multipliers *pricing.PriceMultipliers
								if channelMult := upstream.GetPriceMultipliers(pricingModel); channelMult != nil {
									multipliers = &pricing.PriceMultipliers{
										InputMultiplier:         channelMult.GetEffectiveMultiplier("input"),
										OutputMultiplier:        channelMult.GetEffectiveMultiplier("output"),
										CacheCreationMultiplier: channelMult.GetEffectiveMultiplier("cacheCreation"),
										CacheReadMultiplier:     channelMult.GetEffectiveMultiplier("cacheRead"),
									}
								}
								breakdown := pm.CalculateCostWithBreakdown(
									pricingModel,
									usage.InputTokens,
									usage.OutputTokens,
									usage.CacheCreationInputTokens,
									usage.CacheReadInputTokens,
									multipliers,
								)
								record.Price = breakdown.TotalCost
								record.InputCost = breakdown.InputCost
								record.OutputCost = breakdown.OutputCost
								record.CacheCreationCost = breakdown.CacheCreationCost
								record.CacheReadCost = breakdown.CacheReadCost
							}
						}

						if err := reqLogManager.Update(requestLogID, record); err != nil {
							log.Printf("âš ï¸ è¯·æ±‚æ—¥å¿—æ›´æ–°å¤±è´¥: %v", err)
						}

						// Save debug log if enabled (use logBuffer for stream response body)
						SaveDebugLog(c, cfgManager, reqLogManager, requestLogID, resp.StatusCode, resp.Header, logBuffer.Bytes())

						// Track usage for quota (stream responses are successful when channel closed)
						trackMessagesUsage(usageManager, upstream, requestModel, record.Price)
					}
					return
				}

			// ç¼“å­˜äº‹ä»¶ç”¨äºæœ€åçš„æ—¥å¿—è¾“å‡ºå’Œ usage æå–
			if streamLoggingEnabled || needsSynthesizer || debugLogEnabled {
				if streamLoggingEnabled || debugLogEnabled {
					logBuffer.WriteString(event)
				}
				if synthesizer != nil {
					lines := strings.Split(event, "\n")
					for _, line := range lines {
						synthesizer.ProcessLine(line)
					}
				}
			}

			// å®æ—¶è½¬å‘ç»™å®¢æˆ·ç«¯ï¼ˆæµå¼ä¼ è¾“ï¼‰
			if !clientGone {
				_, err := w.Write([]byte(event))
				if err != nil {
					clientGone = true // æ ‡è®°å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œåœæ­¢åç»­å†™å…¥
					errMsg := err.Error()
					if strings.Contains(errMsg, "broken pipe") || strings.Contains(errMsg, "connection reset") {
						if envCfg.ShouldLog("info") {
							log.Printf("â„¹ï¸ å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥ (æ­£å¸¸è¡Œä¸º)ï¼Œç»§ç»­æ¥æ”¶ä¸Šæ¸¸æ•°æ®...")
						}
					} else {
						log.Printf("âš ï¸ æµå¼ä¼ è¾“å†™å…¥é”™è¯¯: %v", err)
					}
					// æ³¨æ„ï¼šè¿™é‡Œä¸å†returnï¼Œè€Œæ˜¯ç»§ç»­å¾ªç¯ä»¥è€—å°½eventChan
				} else {
					flusher.Flush()
				}
			}

		case err, ok := <-errChan:
			if !ok {
				// errChanå…³é—­ï¼Œä½†è¿™ä¸ä¸€å®šæ„å‘³ç€æµç»“æŸï¼Œç»§ç»­ç­‰å¾…eventChan
				continue
			}
				if err != nil {
					// çœŸçš„æœ‰é”™è¯¯å‘ç”Ÿ
					log.Printf("ğŸ’¥ æµå¼ä¼ è¾“é”™è¯¯: %v", err)

				// æ‰“å°å·²æ¥æ”¶åˆ°çš„éƒ¨åˆ†å“åº”
					if envCfg.EnableResponseLogs && envCfg.IsDevelopment() {
						if synthesizer != nil {
						synthesizedContent := synthesizer.GetSynthesizedContent()
						if synthesizedContent != "" && !synthesizer.IsParseFailed() {
							log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åˆæˆå†…å®¹ (éƒ¨åˆ†):\n%s", strings.TrimSpace(synthesizedContent))
						} else if logBuffer.Len() > 0 {
							log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åŸå§‹å†…å®¹ (éƒ¨åˆ†):\n%s", logBuffer.String())
						}
						}
					}
					if reqLogManager != nil && requestLogID != "" {
						completeTime := time.Now()
						record := &requestlog.RequestLog{
							Status:        requestlog.StatusError,
							CompleteTime:  completeTime,
							DurationMs:    completeTime.Sub(startTime).Milliseconds(),
							Type:          upstream.ServiceType,
							ProviderName:  upstream.Name,
							HTTPStatus:    resp.StatusCode,
							ChannelID:     upstream.Index,
							ChannelName:   upstream.Name,
							Error:         err.Error(),
						}
						_ = reqLogManager.Update(requestLogID, record)
					}
					return
				}
			}
		}
	}

// shouldRetryWithNextKey åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ä¸‹ä¸€ä¸ªå¯†é’¥é‡è¯•
// è¿”å›: (shouldFailover bool, isQuotaRelated bool)
func shouldRetryWithNextKey(statusCode int, bodyBytes []byte) (bool, bool) {
	// 401/403 é€šå¸¸æ˜¯è®¤è¯é—®é¢˜
	if statusCode == 401 || statusCode == 403 {
		return true, false
	}

	// 429 é€Ÿç‡é™åˆ¶ï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªå¯†é’¥
	if statusCode == 429 {
		return true, true
	}

	isQuotaRelated := false

	// æ£€æŸ¥é”™è¯¯æ¶ˆæ¯
	var errResp map[string]interface{}
	if err := json.Unmarshal(bodyBytes, &errResp); err == nil {
		if errObj, ok := errResp["error"].(map[string]interface{}); ok {
			if msg, ok := errObj["message"].(string); ok {
				msgLower := strings.ToLower(msg)
				if strings.Contains(msgLower, "insufficient") ||
					strings.Contains(msgLower, "invalid") ||
					strings.Contains(msgLower, "unauthorized") ||
					strings.Contains(msgLower, "quota") ||
					strings.Contains(msgLower, "rate limit") ||
					strings.Contains(msg, "è¯·æ±‚æ•°é™åˆ¶") ||
					strings.Contains(msgLower, "credit") ||
					strings.Contains(msgLower, "balance") {

					// åˆ¤æ–­æ˜¯å¦ä¸ºé¢åº¦/ä½™é¢ç›¸å…³
					if strings.Contains(msgLower, "ç§¯åˆ†ä¸è¶³") ||
						strings.Contains(msgLower, "insufficient") ||
						strings.Contains(msgLower, "credit") ||
						strings.Contains(msgLower, "balance") ||
						strings.Contains(msgLower, "quota") ||
						strings.Contains(msg, "è¯·æ±‚æ•°é™åˆ¶") {
						isQuotaRelated = true
					}
					return true, isQuotaRelated
				}
			}

			if errType, ok := errObj["type"].(string); ok {
				errTypeLower := strings.ToLower(errType)
				if strings.Contains(errTypeLower, "permission") ||
					strings.Contains(errTypeLower, "insufficient") ||
					strings.Contains(errTypeLower, "over_quota") ||
					strings.Contains(errTypeLower, "billing") {

					// åˆ¤æ–­æ˜¯å¦ä¸ºé¢åº¦/ä½™é¢ç›¸å…³
					if strings.Contains(errTypeLower, "over_quota") ||
						strings.Contains(errTypeLower, "billing") ||
						strings.Contains(errTypeLower, "insufficient") {
						isQuotaRelated = true
					}
					return true, isQuotaRelated
				}
			}
		}
	}

	// 500+ é”™è¯¯ä¹Ÿå¯ä»¥å°è¯• failover
	if statusCode >= 500 {
		return true, false
	}

	return false, false
}

// maskAPIKey æ©ç APIå¯†é’¥ï¼ˆä¸ TS ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
func maskAPIKey(key string) string {
	if key == "" {
		return ""
	}

	length := len(key)
	if length <= 10 {
		// çŸ­å¯†é’¥ï¼šä¿ç•™å‰3ä½å’Œå2ä½
		if length <= 5 {
			return "***"
		}
		return key[:3] + "***" + key[length-2:]
	}

	// é•¿å¯†é’¥ï¼šä¿ç•™å‰8ä½å’Œå5ä½
	return key[:8] + "***" + key[length-5:]
}
