package scheduler

import (
	"context"
	"fmt"
	"log"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/JillVernus/cc-bridge/internal/config"
	"github.com/JillVernus/cc-bridge/internal/metrics"
	"github.com/JillVernus/cc-bridge/internal/session"
)

// SuspensionChecker interface for checking channel suspension status
type SuspensionChecker interface {
	IsChannelSuspended(channelID int, channelType string) (bool, time.Time, string)
}

// ChannelScheduler å¤šæ¸ é“è°ƒåº¦å™¨
type ChannelScheduler struct {
	mu                      sync.RWMutex
	configManager           *config.ConfigManager
	messagesMetricsManager  *metrics.MetricsManager // Messages æ¸ é“æŒ‡æ ‡
	responsesMetricsManager *metrics.MetricsManager // Responses æ¸ é“æŒ‡æ ‡
	traceAffinity           *session.TraceAffinityManager
	suspensionChecker       SuspensionChecker // For checking quota channel suspension
}

// NewChannelScheduler åˆ›å»ºå¤šæ¸ é“è°ƒåº¦å™¨
func NewChannelScheduler(
	cfgManager *config.ConfigManager,
	messagesMetrics *metrics.MetricsManager,
	responsesMetrics *metrics.MetricsManager,
	traceAffinity *session.TraceAffinityManager,
) *ChannelScheduler {
	return &ChannelScheduler{
		configManager:           cfgManager,
		messagesMetricsManager:  messagesMetrics,
		responsesMetricsManager: responsesMetrics,
		traceAffinity:           traceAffinity,
	}
}

// SetSuspensionChecker sets the suspension checker (called after requestLogManager is initialized)
func (s *ChannelScheduler) SetSuspensionChecker(checker SuspensionChecker) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.suspensionChecker = checker
}

// getMetricsManager æ ¹æ®ç±»å‹è·å–å¯¹åº”çš„æŒ‡æ ‡ç®¡ç†å™¨
func (s *ChannelScheduler) getMetricsManager(isResponses bool) *metrics.MetricsManager {
	if isResponses {
		return s.responsesMetricsManager
	}
	return s.messagesMetricsManager
}

// isChannelSuspended checks if a quota channel is suspended
// Returns (isSuspended, suspendedUntil, reason)
func (s *ChannelScheduler) isChannelSuspended(channelIndex int, isResponses bool) (bool, time.Time, string) {
	if s.suspensionChecker == nil {
		return false, time.Time{}, ""
	}
	channelType := "messages"
	if isResponses {
		channelType = "responses"
	}
	return s.suspensionChecker.IsChannelSuspended(channelIndex, channelType)
}

// SelectionResult æ¸ é“é€‰æ‹©ç»“æœ
type SelectionResult struct {
	Upstream     *config.UpstreamConfig
	ChannelIndex int
	Reason       string // é€‰æ‹©åŸå› ï¼ˆç”¨äºæ—¥å¿—ï¼‰

	// Composite channel fields (populated when routing through a composite channel)
	CompositeUpstream     *config.UpstreamConfig // The composite channel used for routing (nil if direct)
	CompositeChannelIndex int                    // Index of the composite channel (-1 if direct)
	ResolvedModel         string                 // The model name after composite resolution (may be overridden)
	FailoverChain         []string               // Remaining failover chain for sticky composite behavior
	FailoverChainIndex    int                    // Current position in failover chain (0 = primary, 1+ = failover)
}

// SelectChannel é€‰æ‹©æœ€ä½³æ¸ é“
// ä¼˜å…ˆçº§: ä¿ƒé”€æœŸæ¸ é“ > Traceäº²å’Œï¼ˆä¿ƒé”€æ¸ é“å¤±è´¥æ—¶å›é€€ï¼‰ > æ¸ é“ä¼˜å…ˆçº§é¡ºåº
// allowedChannels: API key å…è®¸çš„æ¸ é“ç´¢å¼•åˆ—è¡¨ï¼Œnil è¡¨ç¤ºå…è®¸æ‰€æœ‰æ¸ é“
// model: The requested model name (used for composite channel routing)
func (s *ChannelScheduler) SelectChannel(
	ctx context.Context,
	userID string,
	failedChannels map[int]bool,
	isResponses bool,
	allowedChannels []int,
	model string,
) (*SelectionResult, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	// è·å–æ´»è·ƒæ¸ é“åˆ—è¡¨
	activeChannels := s.getActiveChannels(isResponses)
	if len(activeChannels) == 0 {
		return nil, fmt.Errorf("æ²¡æœ‰å¯ç”¨çš„æ´»è·ƒæ¸ é“")
	}

	// Filter by allowed channels if specified
	if len(allowedChannels) > 0 {
		activeChannels = s.filterByAllowedChannels(activeChannels, allowedChannels)
		if len(activeChannels) == 0 {
			return nil, fmt.Errorf("no available channel (allowed channels: %v)", allowedChannels)
		}
	}

	// è·å–å¯¹åº”ç±»å‹çš„æŒ‡æ ‡ç®¡ç†å™¨
	metricsManager := s.getMetricsManager(isResponses)

	// æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†ç®¡ç†å‘˜æ•…éšœè½¬ç§»è®¾ç½®ï¼ˆå¦‚æœå¯ç”¨åˆ™ç¦ç”¨ç”µè·¯æ–­è·¯å™¨ï¼‰
	cfg := s.configManager.GetConfig()
	useCircuitBreaker := !cfg.Failover.Enabled // Failover.Enabled=true æ—¶ç¦ç”¨ç”µè·¯æ–­è·¯å™¨

	// 0. æ£€æŸ¥ä¿ƒé”€æœŸæ¸ é“ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
	promotedChannel := s.findPromotedChannel(activeChannels, isResponses)
	if promotedChannel != nil && !failedChannels[promotedChannel.Index] {
		// æ£€æŸ¥æ˜¯å¦è¢«æš‚åœ
		if suspended, until, reason := s.isChannelSuspended(promotedChannel.Index, isResponses); suspended {
			log.Printf("â¸ï¸ ä¿ƒé”€æ¸ é“ [%d] %s è¢«æš‚åœ (åŸå› : %s, æ¢å¤: %s)", promotedChannel.Index, promotedChannel.Name, reason, until.Format(time.RFC3339))
		} else if !useCircuitBreaker || metricsManager.IsChannelHealthy(promotedChannel.Index) {
			// ä¿ƒé”€æ¸ é“å­˜åœ¨ä¸”æœªå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦å¥åº·ï¼ˆä»…å½“ç”µè·¯æ–­è·¯å™¨å¯ç”¨æ—¶ï¼‰
			upstream := s.getUpstreamByIndex(promotedChannel.Index, isResponses)
			// Composite channels don't need API keys; regular channels do
			if upstream != nil && (config.IsCompositeChannel(upstream) || len(upstream.APIKeys) > 0) {
				log.Printf("ğŸ‰ ä¿ƒé”€æœŸä¼˜å…ˆé€‰æ‹©æ¸ é“: [%d] %s (user: %s)", promotedChannel.Index, upstream.Name, maskUserID(userID))
				result := &SelectionResult{
					Upstream:     upstream,
					ChannelIndex: promotedChannel.Index,
					Reason:       "promotion_priority",
				}
				resolved, err := s.resolveCompositeChannel(result, model, isResponses, failedChannels, metricsManager, useCircuitBreaker)
				if err == nil {
					return resolved, nil
				}
				log.Printf("âš ï¸ [Composite] Failed to resolve promoted channel [%d] %s: %v", promotedChannel.Index, upstream.Name, err)
				failedChannels[promotedChannel.Index] = true
			} else if upstream != nil {
				log.Printf("âš ï¸ ä¿ƒé”€æ¸ é“ [%d] %s æ— å¯ç”¨å¯†é’¥ï¼Œè·³è¿‡", promotedChannel.Index, upstream.Name)
			}
		} else {
			log.Printf("âš ï¸ ä¿ƒé”€æ¸ é“ [%d] %s ä¸å¥åº·ï¼Œè·³è¿‡", promotedChannel.Index, promotedChannel.Name)
		}
	} else if promotedChannel != nil {
		log.Printf("âš ï¸ ä¿ƒé”€æ¸ é“ [%d] %s å·²åœ¨æœ¬æ¬¡è¯·æ±‚ä¸­å¤±è´¥ï¼Œè·³è¿‡", promotedChannel.Index, promotedChannel.Name)
	}

	// 1. æ£€æŸ¥ Trace äº²å’Œæ€§ï¼ˆä¿ƒé”€æ¸ é“å¤±è´¥æ—¶æˆ–æ— ä¿ƒé”€æ¸ é“æ—¶ï¼‰
	if userID != "" {
		if preferredIdx, ok := s.traceAffinity.GetPreferredChannel(userID); ok {
			for _, ch := range activeChannels {
				if ch.Index == preferredIdx && !failedChannels[preferredIdx] {
					// æ£€æŸ¥æ¸ é“çŠ¶æ€ï¼šåªæœ‰ active çŠ¶æ€æ‰ä½¿ç”¨äº²å’Œæ€§
					if ch.Status != "active" {
						log.Printf("â¸ï¸ è·³è¿‡äº²å’Œæ¸ é“ [%d] %s: çŠ¶æ€ä¸º %s (user: %s)", preferredIdx, ch.Name, ch.Status, maskUserID(userID))
						continue
					}
					// æ£€æŸ¥æ˜¯å¦è¢«æš‚åœ
					if suspended, until, reason := s.isChannelSuspended(preferredIdx, isResponses); suspended {
						log.Printf("â¸ï¸ è·³è¿‡äº²å’Œæ¸ é“ [%d] %s: è¢«æš‚åœ (åŸå› : %s, æ¢å¤: %s, user: %s)", preferredIdx, ch.Name, reason, until.Format(time.RFC3339), maskUserID(userID))
						continue
					}
					// æ£€æŸ¥æ¸ é“æ˜¯å¦å¥åº·ï¼ˆä»…å½“ç”µè·¯æ–­è·¯å™¨å¯ç”¨æ—¶ï¼‰
					if !useCircuitBreaker || metricsManager.IsChannelHealthy(preferredIdx) {
						upstream := s.getUpstreamByIndex(preferredIdx, isResponses)
						if upstream != nil {
							log.Printf("ğŸ¯ Traceäº²å’Œé€‰æ‹©æ¸ é“: [%d] %s (user: %s)", preferredIdx, upstream.Name, maskUserID(userID))
							result := &SelectionResult{
								Upstream:     upstream,
								ChannelIndex: preferredIdx,
								Reason:       "trace_affinity",
							}
							resolved, err := s.resolveCompositeChannel(result, model, isResponses, failedChannels, metricsManager, useCircuitBreaker)
							if err == nil {
								return resolved, nil
							}
							log.Printf("âš ï¸ [Composite] Failed to resolve trace affinity channel [%d] %s: %v", preferredIdx, upstream.Name, err)
							failedChannels[preferredIdx] = true
						}
					}
				}
			}
		}
	}

	// 2. æŒ‰ä¼˜å…ˆçº§éå†æ´»è·ƒæ¸ é“
	for _, ch := range activeChannels {
		// è·³è¿‡æœ¬æ¬¡è¯·æ±‚å·²ç»å¤±è´¥çš„æ¸ é“
		if failedChannels[ch.Index] {
			continue
		}

		// è·³è¿‡é active çŠ¶æ€çš„æ¸ é“ï¼ˆsuspended ç­‰ï¼‰
		if ch.Status != "active" {
			log.Printf("â¸ï¸ è·³è¿‡éæ´»è·ƒæ¸ é“: [%d] %s (çŠ¶æ€: %s)", ch.Index, ch.Name, ch.Status)
			continue
		}

		// è·³è¿‡å¤±è´¥ç‡è¿‡é«˜çš„æ¸ é“ï¼ˆå·²ç†”æ–­æˆ–å³å°†ç†”æ–­ï¼‰- ä»…å½“ç”µè·¯æ–­è·¯å™¨å¯ç”¨æ—¶
		if useCircuitBreaker && !metricsManager.IsChannelHealthy(ch.Index) {
			log.Printf("âš ï¸ è·³è¿‡ä¸å¥åº·æ¸ é“: [%d] %s (å¤±è´¥ç‡: %.1f%%)",
				ch.Index, ch.Name, metricsManager.CalculateFailureRate(ch.Index)*100)
			continue
		}

		// è·³è¿‡è¢«æš‚åœçš„é…é¢æ¸ é“ï¼ˆå› é…é¢è€—å°½ç­‰åŸå› ï¼‰
		if suspended, until, reason := s.isChannelSuspended(ch.Index, isResponses); suspended {
			log.Printf("â¸ï¸ è·³è¿‡æš‚åœæ¸ é“: [%d] %s (åŸå› : %s, æ¢å¤æ—¶é—´: %s)",
				ch.Index, ch.Name, reason, until.Format(time.RFC3339))
			continue
		}

		upstream := s.getUpstreamByIndex(ch.Index, isResponses)
		// Composite channels don't need API keys; regular channels do
		if upstream != nil && (config.IsCompositeChannel(upstream) || len(upstream.APIKeys) > 0) {
			log.Printf("âœ… é€‰æ‹©æ¸ é“: [%d] %s (ä¼˜å…ˆçº§: %d)", ch.Index, upstream.Name, ch.Priority)
			result := &SelectionResult{
				Upstream:     upstream,
				ChannelIndex: ch.Index,
				Reason:       "priority_order",
			}
			resolved, err := s.resolveCompositeChannel(result, model, isResponses, failedChannels, metricsManager, useCircuitBreaker)
			if err == nil {
				return resolved, nil
			}
			log.Printf("âš ï¸ [Composite] Failed to resolve channel [%d] %s: %v", ch.Index, upstream.Name, err)
			failedChannels[ch.Index] = true
			continue
		}
	}

	// 3. æ‰€æœ‰å¥åº·æ¸ é“éƒ½å¤±è´¥ï¼Œé€‰æ‹©å¤±è´¥ç‡æœ€ä½çš„ä½œä¸ºé™çº§
	return s.selectFallbackChannel(activeChannels, failedChannels, isResponses, model, metricsManager, useCircuitBreaker)
}

// findPromotedChannel æŸ¥æ‰¾å¤„äºä¿ƒé”€æœŸçš„æ¸ é“
func (s *ChannelScheduler) findPromotedChannel(activeChannels []ChannelInfo, isResponses bool) *ChannelInfo {
	for i := range activeChannels {
		ch := &activeChannels[i]
		if ch.Status != "active" {
			continue
		}
		upstream := s.getUpstreamByIndex(ch.Index, isResponses)
		if upstream != nil {
			if config.IsChannelInPromotion(upstream) {
				log.Printf("ğŸ‰ æ‰¾åˆ°ä¿ƒé”€æ¸ é“: [%d] %s (promotionUntil: %v)", ch.Index, upstream.Name, upstream.PromotionUntil)
				return ch
			}
		}
	}
	return nil
}

// filterByAllowedChannels filters channels to only those in the allowed list
func (s *ChannelScheduler) filterByAllowedChannels(channels []ChannelInfo, allowed []int) []ChannelInfo {
	if len(allowed) == 0 {
		return channels
	}
	allowedSet := make(map[int]bool)
	for _, idx := range allowed {
		allowedSet[idx] = true
	}
	var filtered []ChannelInfo
	for _, ch := range channels {
		if allowedSet[ch.Index] {
			filtered = append(filtered, ch)
		}
	}
	return filtered
}

// resolveCompositeChannel resolves a composite channel to its target channel based on the model.
// If the selected channel is not composite, it returns the original result unchanged.
// If the selected channel is composite but the target is unavailable, returns an error.
// For composite channels, this finds the first available channel in the failover chain.
func (s *ChannelScheduler) resolveCompositeChannel(
	result *SelectionResult,
	model string,
	isResponses bool,
	failedChannels map[int]bool,
	metricsManager *metrics.MetricsManager,
	useCircuitBreaker bool,
) (*SelectionResult, error) {
	if result == nil || result.Upstream == nil {
		return result, nil
	}

	// Check if this is a composite channel
	if !config.IsCompositeChannel(result.Upstream) {
		// Not composite - return as-is with default composite fields
		result.CompositeChannelIndex = -1
		result.ResolvedModel = model
		return result, nil
	}

	// Get all upstreams for resolution
	cfg := s.configManager.GetConfig()
	var upstreams []config.UpstreamConfig
	if isResponses {
		upstreams = cfg.ResponsesUpstream
	} else {
		upstreams = cfg.Upstream
	}

	composite := result.Upstream
	compositeIndex := result.ChannelIndex

	// Find the mapping for this model pattern
	var matchedMapping *config.CompositeMapping
	for i := range composite.CompositeMappings {
		mapping := &composite.CompositeMappings[i]
		// Check if model contains the pattern (haiku, sonnet, opus)
		if strings.Contains(strings.ToLower(model), strings.ToLower(mapping.Pattern)) {
			matchedMapping = mapping
			break
		}
	}

	if matchedMapping == nil {
		return nil, fmt.Errorf("composite channel '%s' has no mapping for model '%s'", composite.Name, model)
	}

	// Build full channel chain: primary + failoverChain
	fullChain := append([]string{matchedMapping.TargetChannelID}, matchedMapping.FailoverChain...)

	// Determine resolved model (may be overridden by mapping)
	resolvedModel := model
	if matchedMapping.TargetModel != "" {
		resolvedModel = matchedMapping.TargetModel
	}

	// Try each channel in the chain until we find an available one
	// For composite targets, we skip status checks (composite decides routing)
	for chainIdx, channelID := range fullChain {
		targetIndex := -1
		for j := range upstreams {
			if upstreams[j].ID == channelID {
				targetIndex = j
				break
			}
		}

		if targetIndex < 0 || targetIndex >= len(upstreams) {
			log.Printf("âš ï¸ [Composite] '%s' â†’ channel ID '%s' not found, skipping", composite.Name, channelID)
			continue
		}

		targetCopy := upstreams[targetIndex]
		target := &targetCopy

		// For composite channel targets, skip status/suspension/circuit-breaker checks
		// The composite channel decides routing, not the target's status
		if s.isTargetChannelAvailable(target, targetIndex, isResponses, failedChannels, metricsManager, useCircuitBreaker, true) {
			reason := result.Reason + "_via_composite"
			if chainIdx > 0 {
				reason = result.Reason + "_via_composite_failover"
			}
			log.Printf("ğŸ”€ [Composite] '%s' â†’ target [%d] '%s' for model '%s' (chain pos: %d, resolved: '%s')",
				composite.Name, targetIndex, target.Name, model, chainIdx, resolvedModel)
			return &SelectionResult{
				Upstream:              target,
				ChannelIndex:          targetIndex,
				Reason:                reason,
				CompositeUpstream:     composite,
				CompositeChannelIndex: compositeIndex,
				ResolvedModel:         resolvedModel,
				FailoverChain:         fullChain[chainIdx+1:], // Remaining chain for retry
				FailoverChainIndex:    chainIdx,
			}, nil
		}
		log.Printf("âš ï¸ [Composite] '%s' â†’ target [%d] '%s' unavailable (no API keys or already failed), trying next in chain",
			composite.Name, targetIndex, target.Name)
	}

	// All channels in chain exhausted
	return nil, fmt.Errorf("composite channel '%s' exhausted all failover channels for model '%s'", composite.Name, model)
}

// GetNextCompositeFailover returns the next channel in the composite failover chain
// This is called when the current target fails and we need to try the next one
func (s *ChannelScheduler) GetNextCompositeFailover(
	prevResult *SelectionResult,
	isResponses bool,
	failedChannels map[int]bool,
	metricsManager *metrics.MetricsManager,
	useCircuitBreaker bool,
) (*SelectionResult, error) {
	if prevResult == nil || prevResult.CompositeUpstream == nil {
		return nil, fmt.Errorf("not a composite channel result")
	}

	if len(prevResult.FailoverChain) == 0 {
		return nil, fmt.Errorf("composite failover chain exhausted")
	}

	// Get all upstreams
	cfg := s.configManager.GetConfig()
	var upstreams []config.UpstreamConfig
	if isResponses {
		upstreams = cfg.ResponsesUpstream
	} else {
		upstreams = cfg.Upstream
	}

	composite := prevResult.CompositeUpstream
	compositeIndex := prevResult.CompositeChannelIndex

	// Try remaining channels in the failover chain
	for i, channelID := range prevResult.FailoverChain {
		targetIndex := -1
		for j := range upstreams {
			if upstreams[j].ID == channelID {
				targetIndex = j
				break
			}
		}

		if targetIndex < 0 || targetIndex >= len(upstreams) {
			continue
		}

		targetCopy := upstreams[targetIndex]
		target := &targetCopy

		// Skip status checks for composite targets
		if s.isTargetChannelAvailable(target, targetIndex, isResponses, failedChannels, metricsManager, useCircuitBreaker, true) {
			chainPosition := prevResult.FailoverChainIndex + 1 + i
			log.Printf("ğŸ”€ [Composite Failover] '%s' â†’ target [%d] '%s' (chain pos: %d)",
				composite.Name, targetIndex, target.Name, chainPosition)
			return &SelectionResult{
				Upstream:              target,
				ChannelIndex:          targetIndex,
				Reason:                "composite_failover",
				CompositeUpstream:     composite,
				CompositeChannelIndex: compositeIndex,
				ResolvedModel:         prevResult.ResolvedModel,
				FailoverChain:         prevResult.FailoverChain[i+1:],
				FailoverChainIndex:    chainPosition,
			}, nil
		}
	}

	return nil, fmt.Errorf("composite failover chain exhausted for '%s'", composite.Name)
}

// isTargetChannelAvailable checks if a target channel is available for use
// skipStatusCheck: when true, allows disabled/suspended channels (for composite failover chain)
func (s *ChannelScheduler) isTargetChannelAvailable(
	target *config.UpstreamConfig,
	targetIndex int,
	isResponses bool,
	failedChannels map[int]bool,
	metricsManager *metrics.MetricsManager,
	useCircuitBreaker bool,
	skipStatusCheck bool,
) bool {
	// Check if already failed in this request
	if failedChannels[targetIndex] {
		return false
	}

	// Check status (skip for composite failover chain targets)
	if !skipStatusCheck {
		status := config.GetChannelStatus(target)
		if status != "active" {
			return false
		}
	}

	// Check if suspended (skip for composite failover chain targets)
	if !skipStatusCheck {
		if suspended, _, _ := s.isChannelSuspended(targetIndex, isResponses); suspended {
			return false
		}
	}

	// Check circuit breaker health (skip for composite failover chain targets)
	if !skipStatusCheck && useCircuitBreaker && !metricsManager.IsChannelHealthy(targetIndex) {
		return false
	}

	// Check if has API keys
	if len(target.APIKeys) == 0 {
		return false
	}

	return true
}

// selectFallbackChannel é€‰æ‹©é™çº§æ¸ é“ï¼ˆå¤±è´¥ç‡æœ€ä½çš„ï¼‰
func (s *ChannelScheduler) selectFallbackChannel(
	activeChannels []ChannelInfo,
	failedChannels map[int]bool,
	isResponses bool,
	model string,
	metricsManager *metrics.MetricsManager,
	useCircuitBreaker bool,
) (*SelectionResult, error) {
	var bestChannel *ChannelInfo
	bestFailureRate := float64(2) // åˆå§‹åŒ–ä¸ºä¸å¯èƒ½çš„å€¼

	for i := range activeChannels {
		ch := &activeChannels[i]
		if failedChannels[ch.Index] {
			continue
		}
		// è·³è¿‡é active çŠ¶æ€çš„æ¸ é“
		if ch.Status != "active" {
			continue
		}
		// è·³è¿‡è¢«æš‚åœçš„é…é¢æ¸ é“
		if suspended, _, _ := s.isChannelSuspended(ch.Index, isResponses); suspended {
			continue
		}

		failureRate := metricsManager.CalculateFailureRate(ch.Index)
		if failureRate < bestFailureRate {
			bestFailureRate = failureRate
			bestChannel = ch
		}
	}

	if bestChannel != nil {
		upstream := s.getUpstreamByIndex(bestChannel.Index, isResponses)
		if upstream != nil {
			log.Printf("âš ï¸ é™çº§é€‰æ‹©æ¸ é“: [%d] %s (å¤±è´¥ç‡: %.1f%%)",
				bestChannel.Index, upstream.Name, bestFailureRate*100)
			result := &SelectionResult{
				Upstream:     upstream,
				ChannelIndex: bestChannel.Index,
				Reason:       "fallback",
			}
			resolved, err := s.resolveCompositeChannel(result, model, isResponses, failedChannels, metricsManager, useCircuitBreaker)
			if err == nil {
				return resolved, nil
			}
			log.Printf("âš ï¸ [Composite] Failed to resolve fallback channel [%d] %s: %v", bestChannel.Index, upstream.Name, err)
			failedChannels[bestChannel.Index] = true
			// Recursively try to find another fallback
			return s.selectFallbackChannel(activeChannels, failedChannels, isResponses, model, metricsManager, useCircuitBreaker)
		}
	}

	return nil, fmt.Errorf("æ‰€æœ‰æ¸ é“éƒ½ä¸å¯ç”¨")
}

// ChannelInfo æ¸ é“ä¿¡æ¯ï¼ˆç”¨äºæ’åºï¼‰
type ChannelInfo struct {
	Index    int
	Name     string
	Priority int
	Status   string
}

// getActiveChannels è·å–æ´»è·ƒæ¸ é“åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
func (s *ChannelScheduler) getActiveChannels(isResponses bool) []ChannelInfo {
	cfg := s.configManager.GetConfig()

	var upstreams []config.UpstreamConfig
	if isResponses {
		upstreams = cfg.ResponsesUpstream
	} else {
		upstreams = cfg.Upstream
	}

	// ç­›é€‰æ´»è·ƒæ¸ é“
	var activeChannels []ChannelInfo
	for i, upstream := range upstreams {
		status := upstream.Status
		if status == "" {
			status = "active" // é»˜è®¤ä¸ºæ´»è·ƒ
		}

		// åªé€‰æ‹© active çŠ¶æ€çš„æ¸ é“ï¼ˆsuspended ä¹Ÿç®—åœ¨æ´»è·ƒåºåˆ—ä¸­ï¼Œä½†ä¼šè¢«å¥åº·æ£€æŸ¥è¿‡æ»¤ï¼‰
		if status != "disabled" {
			priority := upstream.Priority
			if priority == 0 {
				priority = i // é»˜è®¤ä¼˜å…ˆçº§ä¸ºç´¢å¼•
			}

			activeChannels = append(activeChannels, ChannelInfo{
				Index:    i,
				Name:     upstream.Name,
				Priority: priority,
				Status:   status,
			})
		}
	}

	// æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
	sort.Slice(activeChannels, func(i, j int) bool {
		return activeChannels[i].Priority < activeChannels[j].Priority
	})

	return activeChannels
}

// getUpstreamByIndex æ ¹æ®ç´¢å¼•è·å–ä¸Šæ¸¸é…ç½®
// æ³¨æ„ï¼šè¿”å›çš„æ˜¯å‰¯æœ¬ï¼Œé¿å…æŒ‡å‘ slice å…ƒç´ çš„æŒ‡é’ˆåœ¨ slice é‡åˆ†é…åå¤±æ•ˆ
func (s *ChannelScheduler) getUpstreamByIndex(index int, isResponses bool) *config.UpstreamConfig {
	cfg := s.configManager.GetConfig()

	var upstreams []config.UpstreamConfig
	if isResponses {
		upstreams = cfg.ResponsesUpstream
	} else {
		upstreams = cfg.Upstream
	}

	if index >= 0 && index < len(upstreams) {
		// è¿”å›å‰¯æœ¬ï¼Œé¿å…è¿”å›æŒ‡å‘ slice å…ƒç´ çš„æŒ‡é’ˆ
		upstream := upstreams[index]
		return &upstream
	}
	return nil
}

// RecordSuccess è®°å½•æ¸ é“æˆåŠŸ
func (s *ChannelScheduler) RecordSuccess(channelIndex int, isResponses bool) {
	s.getMetricsManager(isResponses).RecordSuccess(channelIndex)
}

// RecordFailure è®°å½•æ¸ é“å¤±è´¥
func (s *ChannelScheduler) RecordFailure(channelIndex int, isResponses bool) {
	s.getMetricsManager(isResponses).RecordFailure(channelIndex)
}

// SetTraceAffinity è®¾ç½® Trace äº²å’Œ
func (s *ChannelScheduler) SetTraceAffinity(userID string, channelIndex int) {
	if userID != "" {
		s.traceAffinity.SetPreferredChannel(userID, channelIndex)
	}
}

// UpdateTraceAffinity æ›´æ–° Trace äº²å’Œæ—¶é—´ï¼ˆç»­æœŸï¼‰
func (s *ChannelScheduler) UpdateTraceAffinity(userID string) {
	if userID != "" {
		s.traceAffinity.UpdateLastUsed(userID)
	}
}

// GetMessagesMetricsManager è·å– Messages æ¸ é“æŒ‡æ ‡ç®¡ç†å™¨
func (s *ChannelScheduler) GetMessagesMetricsManager() *metrics.MetricsManager {
	return s.messagesMetricsManager
}

// GetResponsesMetricsManager è·å– Responses æ¸ é“æŒ‡æ ‡ç®¡ç†å™¨
func (s *ChannelScheduler) GetResponsesMetricsManager() *metrics.MetricsManager {
	return s.responsesMetricsManager
}

// GetTraceAffinityManager è·å– Trace äº²å’Œæ€§ç®¡ç†å™¨
func (s *ChannelScheduler) GetTraceAffinityManager() *session.TraceAffinityManager {
	return s.traceAffinity
}

// ResetChannelMetrics é‡ç½®æ¸ é“æŒ‡æ ‡ï¼ˆç”¨äºæ¢å¤ç†”æ–­ï¼‰
func (s *ChannelScheduler) ResetChannelMetrics(channelIndex int, isResponses bool) {
	s.getMetricsManager(isResponses).Reset(channelIndex)
}

// GetActiveChannelCount è·å–æ´»è·ƒæ¸ é“æ•°é‡
func (s *ChannelScheduler) GetActiveChannelCount(isResponses bool) int {
	return len(s.getActiveChannels(isResponses))
}

// IsMultiChannelMode åˆ¤æ–­æ˜¯å¦ä¸ºå¤šæ¸ é“æ¨¡å¼
func (s *ChannelScheduler) IsMultiChannelMode(isResponses bool) bool {
	return s.GetActiveChannelCount(isResponses) > 1
}

// maskUserID æ©ç  user_idï¼ˆä¿æŠ¤éšç§ï¼‰
func maskUserID(userID string) string {
	if len(userID) <= 16 {
		return "***"
	}
	return userID[:8] + "***" + userID[len(userID)-4:]
}
