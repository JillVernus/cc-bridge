package scheduler

import (
	"context"
	"errors"
	"fmt"
	"log"
	"math/rand"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/JillVernus/cc-bridge/internal/config"
	"github.com/JillVernus/cc-bridge/internal/metrics"
	"github.com/JillVernus/cc-bridge/internal/session"
)

// ErrNoAllowedChannels is returned when channel permission filtering removes all available channels.
// Handlers should return 403 Forbidden when they receive this error.
var ErrNoAllowedChannels = errors.New("no channels available after permission filtering")

// SuspensionChecker interface for checking channel suspension status
type SuspensionChecker interface {
	IsChannelSuspended(channelID int, channelType string) (bool, time.Time, string)
}

// ChannelScheduler å¤šæ¸ é“è°ƒåº¦å™¨
type ChannelScheduler struct {
	mu                      sync.RWMutex
	configManager           *config.ConfigManager
	messagesMetricsManager  *metrics.MetricsManager // Messages æ¸ é“æŒ‡æ ‡
	responsesMetricsManager *metrics.MetricsManager // Responses æ¸ é“æŒ‡æ ‡
	geminiMetricsManager    *metrics.MetricsManager // Gemini æ¸ é“æŒ‡æ ‡
	traceAffinity           *session.TraceAffinityManager
	suspensionChecker       SuspensionChecker // For checking quota channel suspension
	// Round-robin counters for channel-level load balancing
	messagesRoundRobinCounter  int
	responsesRoundRobinCounter int
	geminiRoundRobinCounter    int
}

// NewChannelScheduler åˆ›å»ºå¤šæ¸ é“è°ƒåº¦å™¨
func NewChannelScheduler(
	cfgManager *config.ConfigManager,
	messagesMetrics *metrics.MetricsManager,
	responsesMetrics *metrics.MetricsManager,
	traceAffinity *session.TraceAffinityManager,
) *ChannelScheduler {
	return &ChannelScheduler{
		configManager:           cfgManager,
		messagesMetricsManager:  messagesMetrics,
		responsesMetricsManager: responsesMetrics,
		geminiMetricsManager:    metrics.NewMetricsManager(), // Create dedicated Gemini metrics
		traceAffinity:           traceAffinity,
	}
}

// SetSuspensionChecker sets the suspension checker (called after requestLogManager is initialized)
func (s *ChannelScheduler) SetSuspensionChecker(checker SuspensionChecker) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.suspensionChecker = checker
}

// getMetricsManager æ ¹æ®ç±»å‹è·å–å¯¹åº”çš„æŒ‡æ ‡ç®¡ç†å™¨
func (s *ChannelScheduler) getMetricsManager(isResponses bool) *metrics.MetricsManager {
	if isResponses {
		return s.responsesMetricsManager
	}
	return s.messagesMetricsManager
}

// isChannelSuspended checks if a quota channel is suspended
// Returns (isSuspended, suspendedUntil, reason)
func (s *ChannelScheduler) isChannelSuspended(channelIndex int, isResponses bool) (bool, time.Time, string) {
	if s.suspensionChecker == nil {
		return false, time.Time{}, ""
	}
	channelType := "messages"
	if isResponses {
		channelType = "responses"
	}
	return s.suspensionChecker.IsChannelSuspended(channelIndex, channelType)
}

// upstreamHasCredentials reports whether the upstream can be used for requests.
// Composite channels are always usable (they route to other channels).
// Regular channels require API keys, except Responses channels of type "openai-oauth" which use OAuth tokens instead.
func (s *ChannelScheduler) upstreamHasCredentials(upstream *config.UpstreamConfig, isResponses bool) bool {
	if upstream == nil {
		return false
	}
	if config.IsCompositeChannel(upstream) {
		return true
	}
	if len(upstream.APIKeys) > 0 {
		return true
	}
	if isResponses && upstream.ServiceType == "openai-oauth" {
		return upstream.OAuthTokens != nil && strings.TrimSpace(upstream.OAuthTokens.AccessToken) != ""
	}
	return false
}

// SelectionResult æ¸ é“é€‰æ‹©ç»“æœ
type SelectionResult struct {
	Upstream     *config.UpstreamConfig
	ChannelIndex int
	Reason       string // é€‰æ‹©åŸå› ï¼ˆç”¨äºæ—¥å¿—ï¼‰

	// Composite channel fields (populated when routing through a composite channel)
	CompositeUpstream     *config.UpstreamConfig // The composite channel used for routing (nil if direct)
	CompositeChannelIndex int                    // Index of the composite channel (-1 if direct)
	ResolvedModel         string                 // The model name after composite resolution (may be overridden)
	FailoverChain         []string               // Remaining failover chain for sticky composite behavior
	FailoverChainIndex    int                    // Current position in failover chain (0 = primary, 1+ = failover)
}

// SelectChannel é€‰æ‹©æœ€ä½³æ¸ é“
// ä¼˜å…ˆçº§: ä¿ƒé”€æœŸæ¸ é“ > Traceäº²å’Œï¼ˆä¿ƒé”€æ¸ é“å¤±è´¥æ—¶å›é€€ï¼‰ > æ¸ é“ä¼˜å…ˆçº§é¡ºåº
// allowedChannels: API key å…è®¸çš„æ¸ é“IDåˆ—è¡¨ï¼Œnil è¡¨ç¤ºå…è®¸æ‰€æœ‰æ¸ é“
// model: The requested model name (used for composite channel routing)
func (s *ChannelScheduler) SelectChannel(
	ctx context.Context,
	userID string,
	failedChannels map[int]bool,
	isResponses bool,
	allowedChannels []string,
	model string,
) (*SelectionResult, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	// è·å–æ´»è·ƒæ¸ é“åˆ—è¡¨
	activeChannels := s.getActiveChannels(isResponses)
	if len(activeChannels) == 0 {
		return nil, fmt.Errorf("no active channels available")
	}

	// Filter by allowed channels if specified
	if len(allowedChannels) > 0 {
		activeChannels = s.filterByAllowedChannels(activeChannels, allowedChannels)
		if len(activeChannels) == 0 {
			return nil, fmt.Errorf("%w (allowed channels: %v)", ErrNoAllowedChannels, allowedChannels)
		}
	}

	// è·å–å¯¹åº”ç±»å‹çš„æŒ‡æ ‡ç®¡ç†å™¨
	metricsManager := s.getMetricsManager(isResponses)

	// æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†ç®¡ç†å‘˜æ•…éšœè½¬ç§»è®¾ç½®ï¼ˆå¦‚æœå¯ç”¨åˆ™ç¦ç”¨ç”µè·¯æ–­è·¯å™¨ï¼‰
	cfg := s.configManager.GetConfig()
	useCircuitBreaker := !cfg.Failover.Enabled // Failover.Enabled=true æ—¶ç¦ç”¨ç”µè·¯æ–­è·¯å™¨

	// 0. æ£€æŸ¥ä¿ƒé”€æœŸæ¸ é“ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
	promotedChannel := s.findPromotedChannel(activeChannels, isResponses)
	if promotedChannel != nil && !failedChannels[promotedChannel.Index] {
		// æ£€æŸ¥æ˜¯å¦è¢«æš‚åœ
		if suspended, until, reason := s.isChannelSuspended(promotedChannel.Index, isResponses); suspended {
			log.Printf("â¸ï¸ ä¿ƒé”€æ¸ é“ [%d] %s è¢«æš‚åœ (åŸå› : %s, æ¢å¤: %s)", promotedChannel.Index, promotedChannel.Name, reason, until.Format(time.RFC3339))
		} else if !useCircuitBreaker || metricsManager.IsChannelHealthy(promotedChannel.Index) {
			// ä¿ƒé”€æ¸ é“å­˜åœ¨ä¸”æœªå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦å¥åº·ï¼ˆä»…å½“ç”µè·¯æ–­è·¯å™¨å¯ç”¨æ—¶ï¼‰
			upstream := s.getUpstreamByIndex(promotedChannel.Index, isResponses)
			if s.upstreamHasCredentials(upstream, isResponses) {
				log.Printf("ğŸ‰ ä¿ƒé”€æœŸä¼˜å…ˆé€‰æ‹©æ¸ é“: [%d] %s (user: %s)", promotedChannel.Index, upstream.Name, maskUserID(userID))
				result := &SelectionResult{
					Upstream:     upstream,
					ChannelIndex: promotedChannel.Index,
					Reason:       "promotion_priority",
				}
				resolved, err := s.resolveCompositeChannel(result, model, isResponses, failedChannels, metricsManager, useCircuitBreaker)
				if err == nil {
					return resolved, nil
				}
				log.Printf("âš ï¸ [Composite] Failed to resolve promoted channel [%d] %s: %v", promotedChannel.Index, upstream.Name, err)
				failedChannels[promotedChannel.Index] = true
			} else if upstream != nil {
				log.Printf("âš ï¸ ä¿ƒé”€æ¸ é“ [%d] %s æ— å¯ç”¨å¯†é’¥ï¼Œè·³è¿‡", promotedChannel.Index, upstream.Name)
			}
		} else {
			log.Printf("âš ï¸ ä¿ƒé”€æ¸ é“ [%d] %s ä¸å¥åº·ï¼Œè·³è¿‡", promotedChannel.Index, promotedChannel.Name)
		}
	} else if promotedChannel != nil {
		log.Printf("âš ï¸ ä¿ƒé”€æ¸ é“ [%d] %s å·²åœ¨æœ¬æ¬¡è¯·æ±‚ä¸­å¤±è´¥ï¼Œè·³è¿‡", promotedChannel.Index, promotedChannel.Name)
	}

	// 1. æ£€æŸ¥ Trace äº²å’Œæ€§ï¼ˆä¿ƒé”€æ¸ é“å¤±è´¥æ—¶æˆ–æ— ä¿ƒé”€æ¸ é“æ—¶ï¼‰
	if userID != "" {
		if preferredIdx, ok := s.traceAffinity.GetPreferredChannel(userID); ok {
			for _, ch := range activeChannels {
				if ch.Index == preferredIdx && !failedChannels[preferredIdx] {
					// æ£€æŸ¥æ¸ é“çŠ¶æ€ï¼šåªæœ‰ active çŠ¶æ€æ‰ä½¿ç”¨äº²å’Œæ€§
					if ch.Status != "active" {
						log.Printf("â¸ï¸ è·³è¿‡äº²å’Œæ¸ é“ [%d] %s: çŠ¶æ€ä¸º %s (user: %s)", preferredIdx, ch.Name, ch.Status, maskUserID(userID))
						continue
					}
					// æ£€æŸ¥æ˜¯å¦è¢«æš‚åœ
					if suspended, until, reason := s.isChannelSuspended(preferredIdx, isResponses); suspended {
						log.Printf("â¸ï¸ è·³è¿‡äº²å’Œæ¸ é“ [%d] %s: è¢«æš‚åœ (åŸå› : %s, æ¢å¤: %s, user: %s)", preferredIdx, ch.Name, reason, until.Format(time.RFC3339), maskUserID(userID))
						continue
					}
					// æ£€æŸ¥æ¸ é“æ˜¯å¦å¥åº·ï¼ˆä»…å½“ç”µè·¯æ–­è·¯å™¨å¯ç”¨æ—¶ï¼‰
					if !useCircuitBreaker || metricsManager.IsChannelHealthy(preferredIdx) {
						upstream := s.getUpstreamByIndex(preferredIdx, isResponses)
						if upstream != nil {
							log.Printf("ğŸ¯ Traceäº²å’Œé€‰æ‹©æ¸ é“: [%d] %s (user: %s)", preferredIdx, upstream.Name, maskUserID(userID))
							result := &SelectionResult{
								Upstream:     upstream,
								ChannelIndex: preferredIdx,
								Reason:       "trace_affinity",
							}
							resolved, err := s.resolveCompositeChannel(result, model, isResponses, failedChannels, metricsManager, useCircuitBreaker)
							if err == nil {
								return resolved, nil
							}
							log.Printf("âš ï¸ [Composite] Failed to resolve trace affinity channel [%d] %s: %v", preferredIdx, upstream.Name, err)
							failedChannels[preferredIdx] = true
						}
					}
				}
			}
		}
	}

	// 2. Build list of healthy candidate channels
	var healthyCandidates []ChannelInfo
	for _, ch := range activeChannels {
		// è·³è¿‡æœ¬æ¬¡è¯·æ±‚å·²ç»å¤±è´¥çš„æ¸ é“
		if failedChannels[ch.Index] {
			continue
		}

		// è·³è¿‡é active çŠ¶æ€çš„æ¸ é“ï¼ˆsuspended ç­‰ï¼‰
		if ch.Status != "active" {
			log.Printf("â¸ï¸ è·³è¿‡éæ´»è·ƒæ¸ é“: [%d] %s (çŠ¶æ€: %s)", ch.Index, ch.Name, ch.Status)
			continue
		}

		// è·³è¿‡å¤±è´¥ç‡è¿‡é«˜çš„æ¸ é“ï¼ˆå·²ç†”æ–­æˆ–å³å°†ç†”æ–­ï¼‰- ä»…å½“ç”µè·¯æ–­è·¯å™¨å¯ç”¨æ—¶
		if useCircuitBreaker && !metricsManager.IsChannelHealthy(ch.Index) {
			log.Printf("âš ï¸ è·³è¿‡ä¸å¥åº·æ¸ é“: [%d] %s (å¤±è´¥ç‡: %.1f%%)",
				ch.Index, ch.Name, metricsManager.CalculateFailureRate(ch.Index)*100)
			continue
		}

		// è·³è¿‡è¢«æš‚åœçš„é…é¢æ¸ é“ï¼ˆå› é…é¢è€—å°½ç­‰åŸå› ï¼‰
		if suspended, until, reason := s.isChannelSuspended(ch.Index, isResponses); suspended {
			log.Printf("â¸ï¸ è·³è¿‡æš‚åœæ¸ é“: [%d] %s (åŸå› : %s, æ¢å¤æ—¶é—´: %s)",
				ch.Index, ch.Name, reason, until.Format(time.RFC3339))
			continue
		}

		upstream := s.getUpstreamByIndex(ch.Index, isResponses)
		if s.upstreamHasCredentials(upstream, isResponses) {
			healthyCandidates = append(healthyCandidates, ch)
		}
	}

	if len(healthyCandidates) == 0 {
		// 3. æ‰€æœ‰å¥åº·æ¸ é“éƒ½å¤±è´¥ï¼Œé€‰æ‹©å¤±è´¥ç‡æœ€ä½çš„ä½œä¸ºé™çº§
		return s.selectFallbackChannel(activeChannels, failedChannels, isResponses, model, metricsManager, useCircuitBreaker)
	}

	// 3. Apply channel selection strategy
	loadBalanceStrategy := cfg.LoadBalance
	if isResponses {
		loadBalanceStrategy = cfg.ResponsesLoadBalance
	}

	// Order candidates based on strategy
	orderedCandidates := s.orderChannelsByStrategy(healthyCandidates, loadBalanceStrategy, isResponses)

	// Try each candidate in order
	for _, ch := range orderedCandidates {
		upstream := s.getUpstreamByIndex(ch.Index, isResponses)
		if upstream == nil {
			continue
		}

		reason := "priority_order"
		switch loadBalanceStrategy {
		case "round-robin":
			reason = "round_robin"
		case "random":
			reason = "random"
		}

		log.Printf("âœ… é€‰æ‹©æ¸ é“: [%d] %s (ç­–ç•¥: %s)", ch.Index, upstream.Name, reason)
		result := &SelectionResult{
			Upstream:     upstream,
			ChannelIndex: ch.Index,
			Reason:       reason,
		}
		resolved, err := s.resolveCompositeChannel(result, model, isResponses, failedChannels, metricsManager, useCircuitBreaker)
		if err == nil {
			return resolved, nil
		}
		log.Printf("âš ï¸ [Composite] Failed to resolve channel [%d] %s: %v", ch.Index, upstream.Name, err)
		failedChannels[ch.Index] = true
	}

	// 4. æ‰€æœ‰å¥åº·æ¸ é“éƒ½å¤±è´¥ï¼Œé€‰æ‹©å¤±è´¥ç‡æœ€ä½çš„ä½œä¸ºé™çº§
	return s.selectFallbackChannel(activeChannels, failedChannels, isResponses, model, metricsManager, useCircuitBreaker)
}

// findPromotedChannel æŸ¥æ‰¾å¤„äºä¿ƒé”€æœŸçš„æ¸ é“
func (s *ChannelScheduler) findPromotedChannel(activeChannels []ChannelInfo, isResponses bool) *ChannelInfo {
	for i := range activeChannels {
		ch := &activeChannels[i]
		if ch.Status != "active" {
			continue
		}
		upstream := s.getUpstreamByIndex(ch.Index, isResponses)
		if upstream != nil {
			if config.IsChannelInPromotion(upstream) {
				log.Printf("ğŸ‰ æ‰¾åˆ°ä¿ƒé”€æ¸ é“: [%d] %s (promotionUntil: %v)", ch.Index, upstream.Name, upstream.PromotionUntil)
				return ch
			}
		}
	}
	return nil
}

// orderChannelsByStrategy orders channels based on the load balancing strategy
// - failover: keep priority order (already sorted)
// - round-robin: rotate starting position based on counter
// - random: shuffle the channels
func (s *ChannelScheduler) orderChannelsByStrategy(channels []ChannelInfo, strategy string, isResponses bool) []ChannelInfo {
	if len(channels) <= 1 {
		return channels
	}

	// Make a copy to avoid modifying the original
	result := make([]ChannelInfo, len(channels))
	copy(result, channels)

	switch strategy {
	case "round-robin":
		// Get and increment the counter (need write lock for this)
		// Note: We're already holding RLock, so we need to be careful
		// For simplicity, use atomic-like behavior by reading counter value
		var counter int
		if isResponses {
			counter = s.responsesRoundRobinCounter
			s.responsesRoundRobinCounter++
		} else {
			counter = s.messagesRoundRobinCounter
			s.messagesRoundRobinCounter++
		}
		// Rotate the slice: start from counter % len position
		startIdx := counter % len(result)
		if startIdx > 0 {
			rotated := make([]ChannelInfo, len(result))
			copy(rotated, result[startIdx:])
			copy(rotated[len(result)-startIdx:], result[:startIdx])
			result = rotated
		}
		log.Printf("ğŸ”„ Round-robin: counter=%d, startIdx=%d, first=[%d] %s",
			counter, startIdx, result[0].Index, result[0].Name)

	case "random":
		// Shuffle using Fisher-Yates algorithm
		for i := len(result) - 1; i > 0; i-- {
			j := rand.Intn(i + 1)
			result[i], result[j] = result[j], result[i]
		}
		log.Printf("ğŸ² Random: first=[%d] %s", result[0].Index, result[0].Name)

	default: // "failover" or any other value - keep priority order
		log.Printf("ğŸ”€ Failover: first=[%d] %s (priority: %d)", result[0].Index, result[0].Name, result[0].Priority)
	}

	return result
}

// filterByAllowedChannels filters channels to only those in the allowed list (by channel ID)
func (s *ChannelScheduler) filterByAllowedChannels(channels []ChannelInfo, allowed []string) []ChannelInfo {
	if len(allowed) == 0 {
		return channels
	}
	allowedSet := make(map[string]bool)
	for _, id := range allowed {
		allowedSet[id] = true
	}
	var filtered []ChannelInfo
	for _, ch := range channels {
		if allowedSet[ch.ID] {
			filtered = append(filtered, ch)
		}
	}
	return filtered
}

// resolveCompositeChannel resolves a composite channel to its target channel based on the model.
// If the selected channel is not composite, it returns the original result unchanged.
// If the selected channel is composite but the target is unavailable, returns an error.
// For composite channels, this finds the first available channel in the failover chain.
func (s *ChannelScheduler) resolveCompositeChannel(
	result *SelectionResult,
	model string,
	isResponses bool,
	failedChannels map[int]bool,
	metricsManager *metrics.MetricsManager,
	useCircuitBreaker bool,
) (*SelectionResult, error) {
	if result == nil || result.Upstream == nil {
		return result, nil
	}

	// Check if this is a composite channel
	if !config.IsCompositeChannel(result.Upstream) {
		// Not composite - return as-is with default composite fields
		result.CompositeChannelIndex = -1
		result.ResolvedModel = model
		return result, nil
	}

	// Get all upstreams for resolution
	cfg := s.configManager.GetConfig()
	var upstreams []config.UpstreamConfig
	if isResponses {
		upstreams = cfg.ResponsesUpstream
	} else {
		upstreams = cfg.Upstream
	}

	composite := result.Upstream
	compositeIndex := result.ChannelIndex

	// Find the mapping for this model pattern
	var matchedMapping *config.CompositeMapping
	for i := range composite.CompositeMappings {
		mapping := &composite.CompositeMappings[i]
		// Check if model contains the pattern (haiku, sonnet, opus)
		if strings.Contains(strings.ToLower(model), strings.ToLower(mapping.Pattern)) {
			matchedMapping = mapping
			break
		}
	}

	if matchedMapping == nil {
		return nil, fmt.Errorf("composite channel '%s' has no mapping for model '%s'", composite.Name, model)
	}

	// Build full channel chain: primary + failoverChain
	fullChain := append([]string{matchedMapping.TargetChannelID}, matchedMapping.FailoverChain...)

	// Determine resolved model (may be overridden by mapping)
	resolvedModel := model
	if matchedMapping.TargetModel != "" {
		resolvedModel = matchedMapping.TargetModel
	}

	// Try each channel in the chain until we find an available one
	// For composite targets, we skip status checks (composite decides routing)
	for chainIdx, channelID := range fullChain {
		targetIndex := -1
		for j := range upstreams {
			if upstreams[j].ID == channelID {
				targetIndex = j
				break
			}
		}

		if targetIndex < 0 || targetIndex >= len(upstreams) {
			log.Printf("âš ï¸ [Composite] '%s' â†’ channel ID '%s' not found, skipping", composite.Name, channelID)
			continue
		}

		targetCopy := upstreams[targetIndex]
		target := &targetCopy

		// For composite channel targets, skip status/suspension/circuit-breaker checks
		// The composite channel decides routing, not the target's status
		if s.isTargetChannelAvailable(target, targetIndex, isResponses, failedChannels, metricsManager, useCircuitBreaker, true) {
			reason := result.Reason + "_via_composite"
			if chainIdx > 0 {
				reason = result.Reason + "_via_composite_failover"
			}
			log.Printf("ğŸ”€ [Composite] '%s' â†’ target [%d] '%s' for model '%s' (chain pos: %d, resolved: '%s')",
				composite.Name, targetIndex, target.Name, model, chainIdx, resolvedModel)
			return &SelectionResult{
				Upstream:              target,
				ChannelIndex:          targetIndex,
				Reason:                reason,
				CompositeUpstream:     composite,
				CompositeChannelIndex: compositeIndex,
				ResolvedModel:         resolvedModel,
				FailoverChain:         fullChain[chainIdx+1:], // Remaining chain for retry
				FailoverChainIndex:    chainIdx,
			}, nil
		}
		log.Printf("âš ï¸ [Composite] '%s' â†’ target [%d] '%s' unavailable (no API keys or already failed), trying next in chain",
			composite.Name, targetIndex, target.Name)
	}

	// All channels in chain exhausted
	return nil, fmt.Errorf("composite channel '%s' exhausted all failover channels for model '%s'", composite.Name, model)
}

// GetNextCompositeFailover returns the next channel in the composite failover chain
// This is called when the current target fails and we need to try the next one
func (s *ChannelScheduler) GetNextCompositeFailover(
	prevResult *SelectionResult,
	isResponses bool,
	failedChannels map[int]bool,
	metricsManager *metrics.MetricsManager,
	useCircuitBreaker bool,
) (*SelectionResult, error) {
	if prevResult == nil || prevResult.CompositeUpstream == nil {
		return nil, fmt.Errorf("not a composite channel result")
	}

	if len(prevResult.FailoverChain) == 0 {
		return nil, fmt.Errorf("composite failover chain exhausted")
	}

	// Get all upstreams
	cfg := s.configManager.GetConfig()
	var upstreams []config.UpstreamConfig
	if isResponses {
		upstreams = cfg.ResponsesUpstream
	} else {
		upstreams = cfg.Upstream
	}

	composite := prevResult.CompositeUpstream
	compositeIndex := prevResult.CompositeChannelIndex

	// Try remaining channels in the failover chain
	for i, channelID := range prevResult.FailoverChain {
		targetIndex := -1
		for j := range upstreams {
			if upstreams[j].ID == channelID {
				targetIndex = j
				break
			}
		}

		if targetIndex < 0 || targetIndex >= len(upstreams) {
			continue
		}

		targetCopy := upstreams[targetIndex]
		target := &targetCopy

		// Skip status checks for composite targets
		if s.isTargetChannelAvailable(target, targetIndex, isResponses, failedChannels, metricsManager, useCircuitBreaker, true) {
			chainPosition := prevResult.FailoverChainIndex + 1 + i
			log.Printf("ğŸ”€ [Composite Failover] '%s' â†’ target [%d] '%s' (chain pos: %d)",
				composite.Name, targetIndex, target.Name, chainPosition)
			return &SelectionResult{
				Upstream:              target,
				ChannelIndex:          targetIndex,
				Reason:                "composite_failover",
				CompositeUpstream:     composite,
				CompositeChannelIndex: compositeIndex,
				ResolvedModel:         prevResult.ResolvedModel,
				FailoverChain:         prevResult.FailoverChain[i+1:],
				FailoverChainIndex:    chainPosition,
			}, nil
		}
	}

	return nil, fmt.Errorf("composite failover chain exhausted for '%s'", composite.Name)
}

// isTargetChannelAvailable checks if a target channel is available for use
// skipStatusCheck: when true, allows disabled/suspended channels (for composite failover chain)
func (s *ChannelScheduler) isTargetChannelAvailable(
	target *config.UpstreamConfig,
	targetIndex int,
	isResponses bool,
	failedChannels map[int]bool,
	metricsManager *metrics.MetricsManager,
	useCircuitBreaker bool,
	skipStatusCheck bool,
) bool {
	// Check if already failed in this request
	if failedChannels[targetIndex] {
		return false
	}

	// Check status (skip for composite failover chain targets)
	if !skipStatusCheck {
		status := config.GetChannelStatus(target)
		if status != "active" {
			return false
		}
	}

	// Check if suspended (skip for composite failover chain targets)
	if !skipStatusCheck {
		if suspended, _, _ := s.isChannelSuspended(targetIndex, isResponses); suspended {
			return false
		}
	}

	// Check circuit breaker health (skip for composite failover chain targets)
	if !skipStatusCheck && useCircuitBreaker && !metricsManager.IsChannelHealthy(targetIndex) {
		return false
	}

	// Check if has usable credentials
	if !s.upstreamHasCredentials(target, isResponses) {
		return false
	}

	return true
}

// selectFallbackChannel é€‰æ‹©é™çº§æ¸ é“ï¼ˆå¤±è´¥ç‡æœ€ä½çš„ï¼‰
func (s *ChannelScheduler) selectFallbackChannel(
	activeChannels []ChannelInfo,
	failedChannels map[int]bool,
	isResponses bool,
	model string,
	metricsManager *metrics.MetricsManager,
	useCircuitBreaker bool,
) (*SelectionResult, error) {
	var bestChannel *ChannelInfo
	var bestUpstream *config.UpstreamConfig
	bestFailureRate := float64(2) // åˆå§‹åŒ–ä¸ºä¸å¯èƒ½çš„å€¼

	for i := range activeChannels {
		ch := &activeChannels[i]
		if failedChannels[ch.Index] {
			continue
		}
		// è·³è¿‡é active çŠ¶æ€çš„æ¸ é“
		if ch.Status != "active" {
			continue
		}
		// è·³è¿‡è¢«æš‚åœçš„é…é¢æ¸ é“
		if suspended, _, _ := s.isChannelSuspended(ch.Index, isResponses); suspended {
			continue
		}

		// è·å–ä¸Šæ¸¸é…ç½®å¹¶æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ API Keyï¼ˆå¤åˆæ¸ é“é™¤å¤–ï¼‰
		upstream := s.getUpstreamByIndex(ch.Index, isResponses)
		if upstream == nil {
			continue
		}
		if !s.upstreamHasCredentials(upstream, isResponses) {
			continue
		}

		failureRate := metricsManager.CalculateFailureRate(ch.Index)
		if failureRate < bestFailureRate {
			bestFailureRate = failureRate
			bestChannel = ch
			bestUpstream = upstream
		}
	}

	if bestChannel != nil && bestUpstream != nil {
		log.Printf("âš ï¸ é™çº§é€‰æ‹©æ¸ é“: [%d] %s (å¤±è´¥ç‡: %.1f%%)",
			bestChannel.Index, bestUpstream.Name, bestFailureRate*100)
		result := &SelectionResult{
			Upstream:     bestUpstream,
			ChannelIndex: bestChannel.Index,
			Reason:       "fallback",
		}
		resolved, err := s.resolveCompositeChannel(result, model, isResponses, failedChannels, metricsManager, useCircuitBreaker)
		if err == nil {
			return resolved, nil
		}
		log.Printf("âš ï¸ [Composite] Failed to resolve fallback channel [%d] %s: %v", bestChannel.Index, bestUpstream.Name, err)
		failedChannels[bestChannel.Index] = true
		// Recursively try to find another fallback
		return s.selectFallbackChannel(activeChannels, failedChannels, isResponses, model, metricsManager, useCircuitBreaker)
	}

	return nil, fmt.Errorf("all channels are unavailable")
}

// ChannelInfo æ¸ é“ä¿¡æ¯ï¼ˆç”¨äºæ’åºï¼‰
type ChannelInfo struct {
	Index    int
	ID       string // Stable channel ID for permission matching
	Name     string
	Priority int
	Status   string
}

// getActiveChannels è·å–æ´»è·ƒæ¸ é“åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
func (s *ChannelScheduler) getActiveChannels(isResponses bool) []ChannelInfo {
	cfg := s.configManager.GetConfig()

	var upstreams []config.UpstreamConfig
	if isResponses {
		upstreams = cfg.ResponsesUpstream
	} else {
		upstreams = cfg.Upstream
	}

	// ç­›é€‰æ´»è·ƒæ¸ é“
	var activeChannels []ChannelInfo
	for i, upstream := range upstreams {
		status := upstream.Status
		if status == "" {
			status = "active" // é»˜è®¤ä¸ºæ´»è·ƒ
		}

		// åªé€‰æ‹© active çŠ¶æ€çš„æ¸ é“ï¼ˆsuspended ä¹Ÿç®—åœ¨æ´»è·ƒåºåˆ—ä¸­ï¼Œä½†ä¼šè¢«å¥åº·æ£€æŸ¥è¿‡æ»¤ï¼‰
		if status != "disabled" {
			priority := upstream.Priority
			if priority == 0 {
				priority = i // é»˜è®¤ä¼˜å…ˆçº§ä¸ºç´¢å¼•
			}

			activeChannels = append(activeChannels, ChannelInfo{
				Index:    i,
				ID:       upstream.ID,
				Name:     upstream.Name,
				Priority: priority,
				Status:   status,
			})
		}
	}

	// æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
	sort.Slice(activeChannels, func(i, j int) bool {
		return activeChannels[i].Priority < activeChannels[j].Priority
	})

	return activeChannels
}

// getUpstreamByIndex æ ¹æ®ç´¢å¼•è·å–ä¸Šæ¸¸é…ç½®
// æ³¨æ„ï¼šè¿”å›çš„æ˜¯å‰¯æœ¬ï¼Œé¿å…æŒ‡å‘ slice å…ƒç´ çš„æŒ‡é’ˆåœ¨ slice é‡åˆ†é…åå¤±æ•ˆ
func (s *ChannelScheduler) getUpstreamByIndex(index int, isResponses bool) *config.UpstreamConfig {
	cfg := s.configManager.GetConfig()

	var upstreams []config.UpstreamConfig
	if isResponses {
		upstreams = cfg.ResponsesUpstream
	} else {
		upstreams = cfg.Upstream
	}

	if index >= 0 && index < len(upstreams) {
		// è¿”å›å‰¯æœ¬ï¼Œé¿å…è¿”å›æŒ‡å‘ slice å…ƒç´ çš„æŒ‡é’ˆ
		upstream := upstreams[index]
		return &upstream
	}
	return nil
}

// RecordSuccess è®°å½•æ¸ é“æˆåŠŸ
func (s *ChannelScheduler) RecordSuccess(channelIndex int, isResponses bool) {
	s.getMetricsManager(isResponses).RecordSuccess(channelIndex)
}

// RecordFailure è®°å½•æ¸ é“å¤±è´¥
func (s *ChannelScheduler) RecordFailure(channelIndex int, isResponses bool) {
	s.getMetricsManager(isResponses).RecordFailure(channelIndex)
}

// SetTraceAffinity è®¾ç½® Trace äº²å’Œ
func (s *ChannelScheduler) SetTraceAffinity(userID string, channelIndex int) {
	if userID != "" {
		s.traceAffinity.SetPreferredChannel(userID, channelIndex)
	}
}

// UpdateTraceAffinity æ›´æ–° Trace äº²å’Œæ—¶é—´ï¼ˆç»­æœŸï¼‰
func (s *ChannelScheduler) UpdateTraceAffinity(userID string) {
	if userID != "" {
		s.traceAffinity.UpdateLastUsed(userID)
	}
}

// GetMessagesMetricsManager è·å– Messages æ¸ é“æŒ‡æ ‡ç®¡ç†å™¨
func (s *ChannelScheduler) GetMessagesMetricsManager() *metrics.MetricsManager {
	return s.messagesMetricsManager
}

// GetResponsesMetricsManager è·å– Responses æ¸ é“æŒ‡æ ‡ç®¡ç†å™¨
func (s *ChannelScheduler) GetResponsesMetricsManager() *metrics.MetricsManager {
	return s.responsesMetricsManager
}

// GetTraceAffinityManager è·å– Trace äº²å’Œæ€§ç®¡ç†å™¨
func (s *ChannelScheduler) GetTraceAffinityManager() *session.TraceAffinityManager {
	return s.traceAffinity
}

// ResetChannelMetrics é‡ç½®æ¸ é“æŒ‡æ ‡ï¼ˆç”¨äºæ¢å¤ç†”æ–­ï¼‰
func (s *ChannelScheduler) ResetChannelMetrics(channelIndex int, isResponses bool) {
	s.getMetricsManager(isResponses).Reset(channelIndex)
}

// GetActiveChannelCount è·å–æ´»è·ƒæ¸ é“æ•°é‡
func (s *ChannelScheduler) GetActiveChannelCount(isResponses bool) int {
	return len(s.getActiveChannels(isResponses))
}

// IsMultiChannelMode åˆ¤æ–­æ˜¯å¦ä¸ºå¤šæ¸ é“æ¨¡å¼
func (s *ChannelScheduler) IsMultiChannelMode(isResponses bool) bool {
	return s.GetActiveChannelCount(isResponses) > 1
}

// maskUserID æ©ç  user_idï¼ˆä¿æŠ¤éšç§ï¼‰
func maskUserID(userID string) string {
	if len(userID) <= 16 {
		return "***"
	}
	return userID[:8] + "***" + userID[len(userID)-4:]
}

// ==================== Gemini Channel Methods ====================

// ChannelInfo for Gemini channels
type GeminiChannelInfo struct {
	Index    int
	Name     string
	Status   string
	Priority int
}

// SelectGeminiChannel é€‰æ‹© Gemini æ¸ é“
func (s *ChannelScheduler) SelectGeminiChannel(
	ctx context.Context,
	failedChannels map[int]bool,
	allowedChannels []string,
) (*SelectionResult, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	// Get active Gemini channels
	activeChannels := s.getActiveGeminiChannels()
	if len(activeChannels) == 0 {
		return nil, fmt.Errorf("no active Gemini channels available")
	}

	// Filter by allowed channels if specified (by channel ID)
	if len(allowedChannels) > 0 {
		allowedSet := make(map[string]bool)
		for _, id := range allowedChannels {
			allowedSet[id] = true
		}
		var filtered []ChannelInfo
		for _, ch := range activeChannels {
			if allowedSet[ch.ID] {
				filtered = append(filtered, ch)
			}
		}
		if len(filtered) == 0 {
			return nil, fmt.Errorf("%w (allowed channels: %v)", ErrNoAllowedChannels, allowedChannels)
		}
		activeChannels = filtered
	}

	// Remove failed channels
	var availableChannels []ChannelInfo
	for _, ch := range activeChannels {
		if !failedChannels[ch.Index] {
			availableChannels = append(availableChannels, ch)
		}
	}
	if len(availableChannels) == 0 {
		return nil, fmt.Errorf("all Gemini channels have failed")
	}

	// Use priority-based selection
	cfg := s.configManager.GetConfig()
	strategy := cfg.GeminiLoadBalance
	if strategy == "" {
		strategy = "failover"
	}
	orderedChannels := s.orderGeminiChannelsByStrategy(availableChannels, strategy)

	// Select first available healthy channel
	for _, ch := range orderedChannels {
		// Check if suspended
		if suspended, _, _ := s.isGeminiChannelSuspended(ch.Index); suspended {
			continue
		}
		// Check if healthy (circuit breaker)
		if !s.geminiMetricsManager.IsChannelHealthy(ch.Index) {
			continue
		}
		upstream := s.getGeminiUpstreamByIndex(ch.Index)
		if upstream != nil && len(upstream.APIKeys) > 0 {
			return &SelectionResult{
				Upstream:     upstream,
				ChannelIndex: ch.Index,
				Reason:       "gemini_priority",
			}, nil
		}
	}

	// Fallback: return first available even if unhealthy
	for _, ch := range orderedChannels {
		upstream := s.getGeminiUpstreamByIndex(ch.Index)
		if upstream != nil && len(upstream.APIKeys) > 0 {
			return &SelectionResult{
				Upstream:     upstream,
				ChannelIndex: ch.Index,
				Reason:       "gemini_fallback",
			}, nil
		}
	}

	return nil, fmt.Errorf("no usable Gemini channel found")
}

// getActiveGeminiChannels returns active Gemini channels
func (s *ChannelScheduler) getActiveGeminiChannels() []ChannelInfo {
	cfg := s.configManager.GetConfig()
	var channels []ChannelInfo

	for i, upstream := range cfg.GeminiUpstream {
		status := config.GetChannelStatus(&upstream)
		if status == "active" || status == "" {
			channels = append(channels, ChannelInfo{
				Index:    i,
				ID:       upstream.ID,
				Name:     upstream.Name,
				Status:   status,
				Priority: config.GetChannelPriority(&upstream, i),
			})
		}
	}

	return channels
}

// getGeminiUpstreamByIndex gets Gemini upstream by index
func (s *ChannelScheduler) getGeminiUpstreamByIndex(index int) *config.UpstreamConfig {
	cfg := s.configManager.GetConfig()
	if index < 0 || index >= len(cfg.GeminiUpstream) {
		return nil
	}
	upstream := cfg.GeminiUpstream[index]
	return &upstream
}

// orderGeminiChannelsByStrategy orders Gemini channels by strategy
func (s *ChannelScheduler) orderGeminiChannelsByStrategy(channels []ChannelInfo, strategy string) []ChannelInfo {
	result := make([]ChannelInfo, len(channels))
	copy(result, channels)

	switch strategy {
	case "round-robin":
		// Rotate based on counter
		if len(result) > 0 {
			s.geminiRoundRobinCounter = (s.geminiRoundRobinCounter + 1) % len(result)
			rotated := make([]ChannelInfo, len(result))
			for i := range result {
				rotated[i] = result[(i+s.geminiRoundRobinCounter)%len(result)]
			}
			result = rotated
		}
	case "random":
		// Shuffle
		rand.Shuffle(len(result), func(i, j int) {
			result[i], result[j] = result[j], result[i]
		})
	default: // "failover" or priority-based
		// Sort by priority (lower = higher priority)
		sort.Slice(result, func(i, j int) bool {
			return result[i].Priority < result[j].Priority
		})
	}

	return result
}

// isGeminiChannelSuspended checks if a Gemini channel is suspended
func (s *ChannelScheduler) isGeminiChannelSuspended(channelIndex int) (bool, time.Time, string) {
	if s.suspensionChecker == nil {
		return false, time.Time{}, ""
	}
	return s.suspensionChecker.IsChannelSuspended(channelIndex, "gemini")
}

// RecordGeminiSuccess records success for Gemini channel
func (s *ChannelScheduler) RecordGeminiSuccess(channelIndex int) {
	s.geminiMetricsManager.RecordSuccess(channelIndex)
}

// RecordGeminiFailure records failure for Gemini channel
func (s *ChannelScheduler) RecordGeminiFailure(channelIndex int) {
	s.geminiMetricsManager.RecordFailure(channelIndex)
}

// ResetGeminiChannelMetrics resets metrics for a Gemini channel
func (s *ChannelScheduler) ResetGeminiChannelMetrics(channelIndex int) {
	s.geminiMetricsManager.Reset(channelIndex)
}

// GetGeminiMetricsManager returns the Gemini metrics manager
func (s *ChannelScheduler) GetGeminiMetricsManager() *metrics.MetricsManager {
	return s.geminiMetricsManager
}

// GetActiveGeminiChannelCount returns the number of active Gemini channels
func (s *ChannelScheduler) GetActiveGeminiChannelCount() int {
	return len(s.getActiveGeminiChannels())
}

// IsGeminiMultiChannelMode returns true if there are multiple active Gemini channels
func (s *ChannelScheduler) IsGeminiMultiChannelMode() bool {
	return s.GetActiveGeminiChannelCount() > 1
}
